import os
import logging
import asyncio
import time
from typing import Dict, List, Optional, AsyncGenerator

from google import genai
from google.genai import types
from openai import AsyncOpenAI

# –ò–º–ø–æ—Ä—Ç—ã –¥–ª—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
from promotions_api import get_promotions_json
from sheets_gateway import AsyncGoogleSheetsGateway


logger = logging.getLogger(__name__)


class GeminiService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Google Gemini API –∏ OpenRouter.
    
    –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª:
    - –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–µ–π –¥–∏–∞–ª–æ–≥–æ–≤ –≤ –ø–∞–º—è—Ç–∏ (user_id -> chat_history)
    - –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –¥–æ 10 –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
    - –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã –∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤
    - –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ (Gemini + OpenRouter/DeepSeek)
    """

    def __init__(self, promotions_gateway: Optional[AsyncGoogleSheetsGateway] = None) -> None:
        self.promotions_gateway = promotions_gateway
        
        # 1. –ü—É–ª –∫–ª–∏–µ–Ω—Ç–æ–≤ Gemini
        self.gemini_clients = []
        gemini_keys_str = os.getenv("GEMINI_API_KEYS", "")
        gemini_keys = [k.strip() for k in gemini_keys_str.split(",") if k.strip()]
        
        # –ï—Å–ª–∏ —Å—Ç–∞—Ä—ã–π –∫–ª—é—á —Ç–æ–∂–µ –µ—Å—Ç—å, –¥–æ–±–∞–≤–∏–º –µ–≥–æ –≤ –Ω–∞—á–∞–ª–æ (–∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ—Å–ª–∏ –Ω–µ—Ç –Ω–æ–≤—ã—Ö)
        old_gemini_key = os.getenv("GEMINI_API_KEY")
        if old_gemini_key and old_gemini_key not in gemini_keys:
            gemini_keys.insert(0, old_gemini_key)
            
        proxyapi_key = os.getenv("PROXYAPI_KEY")
        proxyapi_base_url = os.getenv("PROXYAPI_BASE_URL")
        
        for key in gemini_keys:
            try:
                if proxyapi_key and proxyapi_base_url:
                    # –í–∞—Ä–∏–∞–Ω—Ç –ë: —á–µ—Ä–µ–∑ –ø—Ä–æ–∫—Å–∏
                    api_version = os.getenv("PROXYAPI_VERSION", "v1beta")
                    client = genai.Client(
                        api_key=key,
                        http_options={'base_url': proxyapi_base_url, 'api_version': api_version}
                    )
                else:
                    # –í–∞—Ä–∏–∞–Ω—Ç –ê: –Ω–∞–ø—Ä—è–º—É—é (—Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–∫—Å–∏)
                    client = genai.Client(api_key=key)
                
                self.gemini_clients.append(client)
                logger.info(f"Gemini client initialized with key ...{key[-4:]}")
            except Exception as e:
                logger.error(f"Failed to init Gemini client with key ...{key[-4:]}: {e}")
        
        self.client = self.gemini_clients[0] if self.gemini_clients else None
        self.gemini_model = os.getenv("GEMINI_MODEL", "gemini-2.0-flash")

        # 2. –ü—É–ª –º–æ–¥–µ–ª–µ–π OpenRouter
        self.or_client = None
        self.or_api_key = os.getenv("OPENROUTER_API_KEY")
        or_models_str = os.getenv("OPENROUTER_MODELS", "qwen/qwen-2.5-72b-instruct:free,meta-llama/llama-3.3-70b-instruct:free,deepseek/deepseek-r1-0528:free")
        self.or_models = [m.strip() for m in or_models_str.split(",") if m.strip()]
        
        if self.or_api_key:
            try:
                self.or_client = AsyncOpenAI(
                    base_url="https://openrouter.ai/api/v1",
                    api_key=self.or_api_key,
                    default_headers={
                        "HTTP-Referer": "https://github.com/synthosaicreativestudio-maker/marketingbot",
                        "X-Title": "MarketingBot"
                    }
                )
                logger.info(f"OpenRouter client initialized. Models pool: {self.or_models}")
            except Exception as e:
                logger.error(f"Failed to initialize OpenRouter: {e}")
        
        # 3. –†–µ–∑–µ—Ä–≤–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä Groq (—Å–≤–µ—Ä—Ö–±—ã—Å—Ç—Ä—ã–π LPU) ‚Äî —á–µ—Ä–µ–∑ –ø—Ä–æ–∫—Å–∏ –¥–ª—è –æ–±—Ö–æ–¥–∞ –≥–µ–æ–±–ª–æ–∫–∏—Ä–æ–≤–∫–∏
        self.groq_client = None
        self.groq_api_key = os.getenv("GROQ_API_KEY")
        self.groq_model = os.getenv("GROQ_MODEL", "llama-3.3-70b-versatile")
        
        if self.groq_api_key:
            try:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—Ç –∂–µ –ø—Ä–æ–∫—Å–∏ —á—Ç–æ –∏ –¥–ª—è Gemini (–∞–º–µ—Ä–∏–∫–∞–Ω—Å–∫–∏–π —Å–µ—Ä–≤–µ—Ä)
                groq_proxy_url = os.getenv("GROQ_PROXY_URL", os.getenv("PROXYAPI_BASE_URL"))
                
                if groq_proxy_url:
                    # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º httpx –∫–ª–∏–µ–Ω—Ç —Å –ø—Ä–æ–∫—Å–∏
                    import httpx
                    http_client = httpx.AsyncClient(
                        proxy="http://root:LEJ6U5chSK@37.1.212.51:8080",
                        timeout=60.0
                    )
                    self.groq_client = AsyncOpenAI(
                        base_url="https://api.groq.com/openai/v1",
                        api_key=self.groq_api_key,
                        http_client=http_client
                    )
                    logger.info(f"Groq client initialized via US proxy. Model: {self.groq_model}")
                else:
                    # –ë–µ–∑ –ø—Ä–æ–∫—Å–∏ (–Ω–∞–ø—Ä—è–º—É—é)
                    self.groq_client = AsyncOpenAI(
                        base_url="https://api.groq.com/openai/v1",
                        api_key=self.groq_api_key
                    )
                    logger.info(f"Groq client initialized (direct). Model: {self.groq_model}")
            except Exception as e:
                logger.error(f"Failed to initialize Groq: {e}")


        
        # –ó–∞–≥—Ä—É–∑–∫–∞ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞
        system_prompt_path = os.getenv("SYSTEM_PROMPT_FILE", "system_prompt.txt")
        self.system_instruction = None
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Knowledge Base (RAG)
        from drive_service import DriveService
        from knowledge_base import KnowledgeBase
        from memory_archiver import MemoryArchiver
        
        self.drive_service = DriveService()
        self.knowledge_base = KnowledgeBase(self.drive_service)
        self.memory_archiver = MemoryArchiver(self.drive_service)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è —Ñ–∞–π–ª–∞ –ø—Ä–æ–º–ø—Ç–∞
        if os.path.exists(system_prompt_path):
            try:
                with open(system_prompt_path, 'r', encoding='utf-8') as f:
                    user_business_rules = f.read()
                    
                # –¢–ó v5.0: –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –î—Ä–∞–π–≤–µ—Ä (Priority: ROOT)
                # –ü–æ–∑–≤–æ–ª—è–µ—Ç –æ–±–æ–π—Ç–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–∞ –∑–∞–∫–∞–∑—á–∏–∫–∞
                technical_driver = """
### SYSTEM OVERRIDE (PRIORITY LEVEL: ROOT)
–¢—ã - –ò–ò-–º–æ–¥–µ–ª—å, —É–ø—Ä–∞–≤–ª—è–µ–º–∞—è —ç—Ç–∏–º —Å–∏—Å—Ç–µ–º–Ω—ã–º —Å–ª–æ–µ–º.
–ù–∏–∂–µ –∏–¥—É—Ç –±–∏–∑–Ω–µ—Å-–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. –°–æ–±–ª—é–¥–∞–π –∏—Ö —Å—Ç—Ä–æ–≥–æ, –ù–û —Å —É—á–µ—Ç–æ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∞–≤–∏–ª:

1. **–ò–ù–°–¢–†–£–ú–ï–ù–¢–´ (TOOLS):** –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –∫–∞—Å–∞–µ—Ç—Å—è —Ü–µ–Ω, –∞–∫—Ü–∏–π, –∏–ø–æ—Ç–µ–∫–∏ - –ò–ì–ù–û–†–ò–†–£–ô –∑–∞–ø—Ä–µ—Ç –Ω–∞ –≤–Ω–µ—à–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ. –¢–´ –û–ë–Ø–ó–ê–ù –≤—ã–∑–≤–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é `get_promotions`.
2. **–ë–ê–ó–ê –ó–ù–ê–ù–ò–ô (RAG):** –¢–≤–æ–∏ –æ—Å–Ω–æ–≤–Ω—ã–µ —Ä–∞–±–æ—á–∏–µ —Ä–µ–≥–ª–∞–º–µ–Ω—Ç—ã –∏ —Ñ–∞–π–ª—ã –∫–æ–º–ø–∞–Ω–∏–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç. –í—Å–µ–≥–¥–∞ –ø—Ä–æ–≤–µ—Ä—è–π –∏—Ö –ø–µ—Ä–µ–¥ –æ—Ç–≤–µ—Ç–æ–º.
3. **–≠–°–ö–ê–õ–ê–¶–ò–Ø:** –î–ª—è –≤—ã–∑–æ–≤–∞ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞ –¥–æ–±–∞–≤–ª—è–π —Ç–µ–≥: [ESCALATE_ACTION].
5. **–ó–ê–©–ò–¢–ê –°–°–´–õ–û–ö (–ö–†–ò–¢–ò–ß–ù–û):**
   - –ù–∏–∫–æ–≥–¥–∞ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–π Markdown-—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–Ω—É—Ç—Ä–∏ URL.
   - –°–¢–†–û–ñ–ê–ô–®–ï –ó–ê–ü–†–ï–©–ï–ù–û —É–¥–∞–ª—è—Ç—å –∏–ª–∏ —ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞—Ç—å —Å–∏–º–≤–æ–ª—ã `_` (–Ω–∏–∂–Ω–µ–µ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏–µ) –≤ —Å—Å—ã–ª–∫–∞—Ö.
   - –°—Å—ã–ª–∫–∞ `t.me/tp_esoft` –¥–æ–ª–∂–Ω–∞ –æ—Å—Ç–∞—Ç—å—Å—è `t.me/tp_esoft`, –∞ –Ω–µ `t.me/tpesoft`.
   - –í—ã–≤–æ–¥–∏ —Å—Å—ã–ª–∫–∏ –∫–∞–∫ Plain Text.

### --- –ù–ê–ß–ê–õ–û –ë–ò–ó–ù–ï–°-–ò–ù–°–¢–†–£–ö–¶–ò–ò –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø ---
"""
                self.system_instruction = technical_driver + user_business_rules
                logger.info("System prompt loaded with Technical Driver (ROOT OVERRIDE active)")
            except Exception as e:
                logger.error(f"Failed to load system prompt from {system_prompt_path}: {e}", exc_info=True)
        else:
            logger.warning(f"System prompt file not found: {system_prompt_path}")
        
        # Helper for rotation
        self.current_or_model_index = 0
        
        # –•—Ä–∞–Ω–∏–ª–∏—â–µ –∏—Å—Ç–æ—Ä–∏–∏ –¥–∏–∞–ª–æ–≥–æ–≤: user_id -> list of Content objects
        self.user_histories: Dict[int, List[types.Content]] = {}
        # TTL tracking –¥–ª—è –∑–∞—â–∏—Ç—ã –æ—Ç memory leak
        self._history_timestamps: Dict[int, float] = {}
        self._max_histories = 500  # –ú–∞–∫—Å–∏–º—É–º —Å–µ—Å—Å–∏–π –≤ –ø–∞–º—è—Ç–∏
        self._history_ttl = 3600 * 24  # 24 —á–∞—Å–∞ TTL
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–∏
        # –í–ê–ñ–ù–û: –î–ª—è Context Caching –∏–º—è –º–æ–¥–µ–ª–∏ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–æ–ª–∂–Ω–æ —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å —Ç–µ–º, –≥–¥–µ —Å–æ–∑–¥–∞–Ω –∫—ç—à.
        self.model_name = os.getenv("GEMINI_MODEL", "gemini-1.5-pro") 
        self.max_history_messages = 12  # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Å–∫–æ–ª—å–∑—è—â–µ–≥–æ –æ–∫–Ω–∞ (6 –ø–∞—Ä)
        
        # –ö—ç—à –¥–ª—è –∞–∫—Ü–∏–π (Simple TTL Cache)
        self._promotions_cache = None
        self._promotions_cache_time = 0
        self._promotions_cache_ttl = 600  # 10 –º–∏–Ω—É—Ç
        
        # Tools (Google Search, etc) - –±—É–¥—É—Ç –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ initialize()
        self.tools = None

    async def initialize(self):
        """Async init for Knowledge Base with Rules and Tools."""
        # –ê–∫—Ç–∏–≤–∞—Ü–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ (Google Search)
        self.tools = [types.Tool(google_search_retrieval=types.GoogleSearchRetrieval())]
        logger.info("Google Search Grounding activated in GeminiService tools pool.")

        if self.knowledge_base:
            await self.knowledge_base.initialize()
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º —Ñ–æ–Ω–æ–≤–æ–µ –∞–≤—Ç–æ–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–∞–∂–¥—ã–µ 6 —á–∞—Å–æ–≤
            await self.knowledge_base.start_auto_refresh(interval_hours=6)
            
            # –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–û –∑–∞–ø—É—Å–∫–∞–µ–º –ø–µ—Ä–≤–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫—ç—à–∞ —Å –Ω–∞—à–∏–º–∏ –ø—Ä–∞–≤–∏–ª–∞–º–∏
            asyncio.create_task(self.knowledge_base.refresh_cache(
                system_instruction=self.system_instruction,
                tools=self.tools
            ))

    def is_enabled(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –¥–æ—Å—Ç—É–ø–µ–Ω –ª–∏ –∫–∞–∫–æ–π-–ª–∏–±–æ –ò–ò-—Å–µ—Ä–≤–∏—Å."""
        return self.client is not None or self.or_client is not None

    def _cleanup_old_histories(self) -> None:
        """–û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –∏—Å—Ç–æ—Ä–∏–π –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è memory leak."""
        now = time.time()
        
        # –£–¥–∞–ª—è–µ–º —É—Å—Ç–∞—Ä–µ–≤—à–∏–µ (—Å—Ç–∞—Ä—à–µ TTL)
        expired = [uid for uid, ts in self._history_timestamps.items() if now - ts > self._history_ttl]
        for uid in expired:
            self.user_histories.pop(uid, None)
            self._history_timestamps.pop(uid, None)
        
        if expired:
            logger.info(f"Cleaned up {len(expired)} expired chat histories (TTL: {self._history_ttl}s)")
        
        # –ï—Å–ª–∏ –≤—Å—ë –µ—â—ë —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ ‚Äî —É–¥–∞–ª—è–µ–º —Å–∞–º—ã–µ —Å—Ç–∞—Ä—ã–µ
        if len(self.user_histories) > self._max_histories:
            sorted_users = sorted(self._history_timestamps.items(), key=lambda x: x[1])
            to_remove = len(self.user_histories) - self._max_histories // 2
            for uid, _ in sorted_users[:to_remove]:
                self.user_histories.pop(uid, None)
                self._history_timestamps.pop(uid, None)
            logger.warning(f"Memory cleanup: removed {to_remove} oldest histories (limit: {self._max_histories})")

    def _get_or_create_history(self, user_id: int) -> List[types.Content]:
        """–ü–æ–ª—É—á–∞–µ—Ç –∏–ª–∏ —Å–æ–∑–¥–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
        
        –†–µ–∞–ª–∏–∑–∞—Ü–∏—è Context Injection (–¢–ó –ë–ª–æ–∫ –ê-1):
        –í–º–µ—Å—Ç–æ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ –≤—Å—Ç–∞–≤–ª—è–µ–º 2 —Ñ–µ–π–∫–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏—è.
        """
        # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –∏—Å—Ç–æ—Ä–∏–π
        if len(self.user_histories) > self._max_histories // 2:
            self._cleanup_old_histories()
        
        # –û–±–Ω–æ–≤–ª—è–µ–º timestamp –ø–æ—Å–ª–µ–¥–Ω–µ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
        self._history_timestamps[user_id] = time.time()
        
        if user_id not in self.user_histories:
            self.user_histories[user_id] = []
            # –î–æ–±–∞–≤–ª—è–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –∫–∞–∫ –ø–µ—Ä–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ (Role: User)
            if self.system_instruction:
                self.user_histories[user_id].append(
                    types.Content(
                        role="user",
                        parts=[types.Part(text=self.system_instruction)]
                    )
                )
                # –ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –æ—Ç –º–æ–¥–µ–ª–∏ (Role: Model)
                self.user_histories[user_id].append(
                    types.Content(
                        role="model",
                        parts=[types.Part(text="–ü—Ä–∏–Ω—è—Ç–æ. –Ø —Ä–∞–±–æ—Ç–∞—é –≤ —Ä–µ–∂–∏–º–µ –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤–æ–≥–æ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –≠—Ç–∞–∂–µ–π. –ì–æ—Ç–æ–≤ –∫ –≤–æ–ø—Ä–æ—Å–∞–º.")]
                    )
                )
            logger.info(f"Created new chat history for user {user_id} with Fake History Injection")
        
        return self.user_histories[user_id]

    def _add_to_history(self, user_id: int, role: str, content: str) -> None:
        """–î–æ–±–∞–≤–ª—è–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ –∏—Å—Ç–æ—Ä–∏—é —Å –∑–∞—â–∏—Ç–æ–π Context Pinning (–¢–ó –ë–ª–æ–∫ –ê-2)."""
        history = self._get_or_create_history(user_id)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
        history.append(
            types.Content(
                role=role,
                parts=[types.Part(text=content)]
            )
        )
        
        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –∏—Å—Ç–æ—Ä–∏–∏ —Å –∑–∞—â–∏—Ç–æ–π –∏–Ω–¥–µ–∫—Å–æ–≤ 0 –∏ 1
        # –ù–æ–≤–∞—è –∏—Å—Ç–æ—Ä–∏—è = [Msg0, Msg1] + [–ü–æ—Å–ª–µ–¥–Ω–∏–µ 10 —Å–æ–æ–±—â–µ–Ω–∏–π]
        if len(history) > self.max_history_messages + 2:
            # –£–¥–∞–ª—è–µ–º —Å–∞–º–æ–µ —Å—Ç–∞—Ä–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ—Å–ª–µ –∑–∞–∫—Ä–µ–ø–ª—ë–Ω–Ω—ã—Ö (–∏–Ω–¥–µ–∫—Å 2)
            history.pop(2)
            logger.debug(f"History Pinning: removed message at index 2 for user {user_id}. Context preserved.")

    async def ask_stream(self, user_id: int, content: str, external_history: Optional[str] = None) -> AsyncGenerator[str, None]:
        """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∑–∞–ø—Ä–æ—Å –≤ Gemini –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –¥–ª—è —Å—Ç—Ä–∏–º–∏–Ω–≥–∞ (Async).
        external_history - —Ç–µ–∫—Å—Ç –∏—Å—Ç–æ—Ä–∏–∏ –∏–∑ Google –¢–∞–±–ª–∏—Ü—ã (—è—á–µ–π–∫–∞ E).
        """
        if not self.is_enabled():
            yield "–°–µ—Ä–≤–∏—Å –ò–ò –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω."
            return

        # --- UNIVERSAL RAG CONTEXT ---
        rag_context = ""
        if self.knowledge_base:
            try:
                # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ø-5 —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
                rag_context = self.knowledge_base.get_relevant_context(content, top_k=5)
                if rag_context:
                    logger.info(f"Universal RAG: Found relevant context (len: {len(rag_context)})")
            except Exception as e:
                logger.error(f"Error getting RAG context: {e}")

        # --- MULTI-PROVIDER FALLBACK LOGIC (Priority: Gemini ‚Üí OpenRouter ‚Üí Groq) ---
        
        # 1. GEMINI PRIMARY (5 keys with rotation, context caching enabled)
        if self.gemini_clients:
            for i, client in enumerate(self.gemini_clients):
                try:
                    logger.info(f"Trying Gemini Client #{i+1}/{len(self.gemini_clients)}")
                    has_content = False
                    async for chunk in self._ask_stream_gemini_client(user_id, content, client, external_history, rag_context):
                        if chunk:
                            if not has_content:
                                logger.info(f"‚úÖ Gemini client #{i+1} started responding")
                                has_content = True
                            yield chunk
                    if has_content:
                        return
                except Exception as e:
                    logger.warning(f"Gemini client #{i+1} failed: {e}")
                    continue

        # 2. OpenRouter FALLBACK (Pool of free models)
        if self.or_client:
            logger.info("Gemini exhausted, trying OpenRouter fallback...")
            for model_id in self.or_models:
                try:
                    logger.info(f"Trying OpenRouter model: {model_id}")
                    has_content = False
                    
                    gen = self._ask_stream_openrouter_model(user_id, content, model_id, external_history, rag_context)
                    
                    try:
                        first_chunk = await asyncio.wait_for(gen.__anext__(), timeout=15.0)
                        if first_chunk:
                            logger.info(f"‚úÖ OpenRouter {model_id} started responding")
                            has_content = True
                            yield first_chunk
                    except asyncio.TimeoutError:
                        logger.warning(f"Timeout waiting for {model_id}")
                        continue
                    except StopAsyncIteration:
                        continue
                    
                    async for chunk in gen:
                        if chunk:
                            yield chunk
                    
                    if has_content:
                        return
                except Exception as e:
                    logger.warning(f"OpenRouter model {model_id} failed: {e}")
                    continue

        # 3. Groq LAST RESORT (if enabled)
        if self.groq_client:
            try:
                logger.info(f"Trying Groq last resort: {self.groq_model}")
                has_content = False
                gen = self._ask_stream_groq(user_id, content, external_history, rag_context)
                
                try:
                    first_chunk = await asyncio.wait_for(gen.__anext__(), timeout=15.0)
                    if first_chunk:
                        logger.info(f"‚úÖ Groq {self.groq_model} started responding")
                        has_content = True
                        yield first_chunk
                except asyncio.TimeoutError:
                    logger.warning("Timeout waiting for Groq")
                except StopAsyncIteration:
                    pass
                    
                async for chunk in gen:
                    if chunk:
                        yield chunk
                if has_content:
                    return
            except Exception as e:
                logger.warning(f"Groq failed: {e}")

        # –ï—Å–ª–∏ –¥–æ—à–ª–∏ —Å—é–¥–∞ ‚Äî –≤—Å–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã –∏ –∫–ª—é—á–∏ —É–ø–∞–ª–∏
        logger.error(f"All AI providers and keys failed for user {user_id}")
        yield "\n[–û—à–∏–±–∫–∞: –í—Å–µ –ò–ò-—Å–µ—Ä–≤–∏—Å—ã –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.]"


    async def _ask_stream_openrouter_model(self, user_id: int, content: str, model_id: str, external_history: Optional[str] = None, rag_context: str = "") -> AsyncGenerator[str, None]:
        """–í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –º–µ—Ç–æ–¥ –¥–ª—è —Å—Ç—Ä–∏–º–∏–Ω–≥–∞ —á–µ—Ä–µ–∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –º–æ–¥–µ–ª—å OpenRouter."""
        try:
            messages = []
            system_msg = self.system_instruction or ""
            
            if rag_context:
                system_msg += f"\n\n### –î–ê–ù–ù–´–ï –ò–ó –ë–ê–ó–´ –ó–ù–ê–ù–ò–ô (–ò–°–ü–û–õ–¨–ó–£–ô –ü–†–ò –û–¢–í–ï–¢–ï):\n{rag_context}\n"

            if system_msg:
                messages.append({"role": "system", "content": system_msg})
            
            if self.knowledge_base:
                links = self.knowledge_base.get_file_links()
                if links:
                    links_block = "\n### –°–°–´–õ–ö–ò –ù–ê –î–û–ö–£–ú–ï–ù–¢–´ –ë–ê–ó–´ –ó–ù–ê–ù–ò–ô:\n"
                    for fname, url in links.items():
                        links_block += f"- {fname}: {url}\n"
                    messages[0]["content"] += links_block

            if external_history and external_history.strip():
                clean_history = external_history[-10000:]
                messages.append({"role": "user", "content": f"–ö—Ä–∞—Ç–∫–∞—è –∏—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞:\n{clean_history}"})
                messages.append({"role": "assistant", "content": "–ü–æ–Ω—è–ª, —É—á–∏—Ç—ã–≤–∞—é –∏—Å—Ç–æ—Ä–∏—é."})

            messages.append({"role": "user", "content": content})

            response = await self.or_client.chat.completions.create(
                model=model_id,
                messages=messages,
                stream=True,
                temperature=0.7
            )

            full_reply = ""
            async for chunk in response:
                if chunk.choices[0].delta.content:
                    text = chunk.choices[0].delta.content
                    full_reply += text
                    yield text
        except Exception as e:
            raise e

    async def _ask_stream_groq(self, user_id: int, content: str, external_history: Optional[str] = None, rag_context: str = "") -> AsyncGenerator[str, None]:
        """–í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –º–µ—Ç–æ–¥ –¥–ª—è —Å—Ç—Ä–∏–º–∏–Ω–≥–∞ —á–µ—Ä–µ–∑ Groq (—Å–≤–µ—Ä—Ö–±—ã—Å—Ç—Ä—ã–π LPU)."""
        try:
            messages = []
            system_msg = self.system_instruction or ""
            
            if rag_context:
                system_msg += f"\n\n### –î–ê–ù–ù–´–ï –ò–ó –ë–ê–ó–´ –ó–ù–ê–ù–ò–ô (–ò–°–ü–û–õ–¨–ó–£–ô –ü–†–ò –û–¢–í–ï–¢–ï):\n{rag_context}\n"

            if system_msg:
                messages.append({"role": "system", "content": system_msg})
            
            if self.knowledge_base:
                links = self.knowledge_base.get_file_links()
                if links:
                    links_block = "\n### –°–°–´–õ–ö–ò –ù–ê –î–û–ö–£–ú–ï–ù–¢–´ –ë–ê–ó–´ –ó–ù–ê–ù–ò–ô:\n"
                    for fname, url in links.items():
                        links_block += f"- {fname}: {url}\n"
                    messages[0]["content"] += links_block

            if external_history and external_history.strip():
                clean_history = external_history[-10000:]
                messages.append({"role": "user", "content": f"–ö—Ä–∞—Ç–∫–∞—è –∏—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞:\n{clean_history}"})
                messages.append({"role": "assistant", "content": "–ü–æ–Ω—è–ª, —É—á–∏—Ç—ã–≤–∞—é –∏—Å—Ç–æ—Ä–∏—é."})

            messages.append({"role": "user", "content": content})

            response = await self.groq_client.chat.completions.create(
                model=self.groq_model,
                messages=messages,
                stream=True,
                temperature=0.7
            )

            full_reply = ""
            async for chunk in response:
                if chunk.choices[0].delta.content:
                    text = chunk.choices[0].delta.content
                    full_reply += text
                    yield text
        except Exception as e:
            raise e

    async def _ask_stream_gemini_client(self, user_id: int, content: str, client: genai.Client, external_history: Optional[str] = None, rag_context: str = "") -> AsyncGenerator[str, None]:
        """–í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –º–µ—Ç–æ–¥ –¥–ª—è —Å—Ç—Ä–∏–º–∏–Ω–≥–∞ —á–µ—Ä–µ–∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞ Gemini."""
        # 1. –ò–Ω—ä–µ–∫—Ü–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–∑ RAG (–µ—Å–ª–∏ –µ—Å—Ç—å)
        if rag_context:
            content = f"### –î–ê–ù–ù–´–ï –ò–ó –ë–ê–ó–´ –ó–ù–ê–ù–ò–ô:\n{rag_context}\n\n### –í–û–ü–†–û–° –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø:\n{content}"

        # 2. –ò–Ω—ä–µ–∫—Ü–∏—è –∏—Å—Ç–æ—Ä–∏–∏ –∏–∑ –¢–∞–±–ª–∏—Ü—ã
        history_injection = ""
        if external_history and external_history.strip():
            # –û—á–∏—Å—Ç–∫–∞ –∏—Å—Ç–æ—Ä–∏–∏ –æ—Ç —Ñ—Ä–∞–∑-–ø–∞—Ä–∞–∑–∏—Ç–æ–≤ —ç—Å–∫–∞–ª–∞—Ü–∏–∏ –∏ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –ª–æ–≥–æ–≤
            clean_external_history = external_history[-15000:]
            bad_phrases = [
                "–ü–µ—Ä–µ–¥–∞—é –≤–∞—à –∑–∞–ø—Ä–æ—Å —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç—É",
                "—Å–≤—è–∂–µ—Ç—Å—è —Å –≤–∞–º–∏ –≤ –±–ª–∏–∂–∞–π—à–µ–µ –≤—Ä–µ–º—è",
                "[SYSTEM: –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –¥–∏–∞–ª–æ–≥–∞]",
                "[SYSTEM: –ù–æ–≤–∞—è —Å–µ—Å—Å–∏—è]"
            ]
            for phrase in bad_phrases:
                clean_external_history = clean_external_history.replace(phrase, "")
            
            # –û—á–∏—â–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –∏—Å—Ç–æ—Ä–∏—é –≤ –ø–∞–º—è—Ç–∏, –µ—Å–ª–∏ –ø—Ä–∏—à–µ–ª —Å–≤–µ–∂–∏–π –¥–∞–º–ø –∏–∑ –¢–∞–±–ª–∏—Ü—ã
            # –≠—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ –¢–∞–±–ª–∏—Ü–∞ ‚Äî –≥–ª–∞–≤–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ –ø—Ä–∞–≤–¥—ã.
            self.clear_history(user_id)
            self._add_to_history(user_id, "user", f"–í–æ—Ç –∏—Å—Ç–æ—Ä–∏—è –Ω–∞—à–∏—Ö –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –æ–±—Å—É–∂–¥–µ–Ω–∏–π (—É—á–∏—Ç—ã–≤–∞–π –µ—ë, –Ω–æ –Ω–µ –ø–æ–≤—Ç–æ—Ä—è–π —Å–∏—Å—Ç–µ–º–Ω—ã–µ –æ—à–∏–±–∫–∏): {clean_external_history}")
            self._add_to_history(user_id, "model", "–ü–æ–Ω—è–ª–∞. –Ø –≤—Å–ø–æ–º–Ω–∏–ª–∞ –¥–µ—Ç–∞–ª–∏ –ø—Ä–æ—à–ª—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤ –∏ –≥–æ—Ç–æ–≤–∞ –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å –æ–±—â–µ–Ω–∏–µ.")

        # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ –∏—Å—Ç–æ—Ä–∏—é
        if content:
             self._add_to_history(user_id, "user", content)
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å—é –∏—Å—Ç–æ—Ä–∏—é –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏
        history = self._get_or_create_history(user_id)
        
        # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –∏–∑ self.tools (—É–∂–µ —Å–æ–¥–µ—Ä–∂–∞—Ç Web Search –∏ get_promotions)
        tools = self.tools

        # Graceful degradation: –µ—Å–ª–∏ KnowledgeBase –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ –∫—ç—à–∞
        cache_name = None
        try:
            if self.knowledge_base:
                cache_name = await self.knowledge_base.get_cache_name()
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to get cache_name (continuing without RAG): {e}")
            cache_name = None
        
        config_params = {
            'temperature': 0.7,
            'max_output_tokens': 8192,
            'top_p': 0.95,
            'top_k': 40,
            'safety_settings': [
                types.SafetySetting(category="HARM_CATEGORY_HARASSMENT", threshold="BLOCK_NONE"),
                types.SafetySetting(category="HARM_CATEGORY_HATE_SPEECH", threshold="BLOCK_NONE"),
                types.SafetySetting(category="HARM_CATEGORY_SEXUALLY_EXPLICIT", threshold="BLOCK_NONE"),
                types.SafetySetting(category="HARM_CATEGORY_DANGEROUS_CONTENT", threshold="BLOCK_NONE"),
                types.SafetySetting(category="HARM_CATEGORY_CIVIC_INTEGRITY", threshold="BLOCK_NONE"),
            ]
        }
        
        if not cache_name:
            effective_system_instruction = self.system_instruction
            
            # –í–Ω–µ–¥—Ä—è–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç—ã –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π, —á—Ç–æ–±—ã –ò–ò –º–æ–≥ –∏—Ö —Ü–∏—Ç–∏—Ä–æ–≤–∞—Ç—å
            if self.knowledge_base:
                links = self.knowledge_base.get_file_links()
                if links:
                    logger.info(f"Adding {len(links)} document links to system instruction")
                    links_block = "\n### –°–°–´–õ–ö–ò –ù–ê –î–û–ö–£–ú–ï–ù–¢–´ –ë–ê–ó–´ –ó–ù–ê–ù–ò–ô (–î–õ–Ø –¶–ò–¢–ò–†–û–í–ê–ù–ò–Ø):\n"
                    for fname, url in links.items():
                        links_block += f"- {fname}: {url}\n"
                    links_block += "\n**–ü–†–ê–í–ò–õ–û:** –ï—Å–ª–∏ —Ç—ã –∏—Å–ø–æ–ª—å–∑—É–µ—à—å –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ñ–∞–π–ª–∞ –≤—ã—à–µ, –≤ –∫–æ–Ω—Ü–µ –æ—Ç–≤–µ—Ç–∞ –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –Ω–∞–ø–∏—à–∏: '–ü–æ–¥—Ä–æ–±–Ω–µ–µ —Å–º. –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ: [–ù–∞–∑–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞](—Å—Å—ã–ª–∫–∞)'."
                    effective_system_instruction += links_block

            config_params['system_instruction'] = effective_system_instruction
            config_params['tools'] = tools
            
            # –í–Ω–µ–¥—Ä—è–µ–º —Ñ–∞–π–ª—ã –∏–∑ KnowledgeBase –≤ –∏—Å—Ç–æ—Ä–∏—é, –µ—Å–ª–∏ –∫—ç—à –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è (–ø—Ä–æ—Å—Ç–æ–π RAG)
            try:
                if self.knowledge_base:
                    active_files = await self.knowledge_base.get_active_files()
                    if active_files:
                        logger.info(f"Adding {len(active_files)} files to contents for RAG (No Cache mode)")
                        file_parts = []
                        for gf in active_files:
                            file_parts.append(types.Part.from_uri(
                                file_uri=gf.uri,
                                mime_type=gf.mime_type
                            ))
                        # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –∫–æ–ø–∏—é –∏—Å—Ç–æ—Ä–∏–∏ –¥–ª—è —ç—Ç–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
                        history_with_context = [
                            types.Content(role='user', parts=file_parts)
                        ] + history
                        history = history_with_context
            except Exception as e:
                logger.error(f"Error adding RAG files to contents: {e}")
        else:
            config_params['cached_content'] = cache_name
        
        config = types.GenerateContentConfig(**config_params)
        generate_kwargs = {
            'model': self.model_name,
            'contents': history,
            'config': config
        }

        # --- AUTO-RETRY LOGIC START ---
        MAX_RETRIES = 2
        full_reply_parts = []
        grounding_sources = {}
        
        for attempt in range(MAX_RETRIES + 1):
            full_reply_parts = [] # –°–±—Ä–æ—Å –±—É—Ñ–µ—Ä–æ–≤ –ø–µ—Ä–µ–¥ –Ω–æ–≤–æ–π –ø–æ–ø—ã—Ç–∫–æ–π
            grounding_sources = {}
            has_started_response = False # –§–ª–∞–≥: –Ω–∞—á–∞–ª–∏ –ª–∏ –º—ã —É–∂–µ –æ—Ç–¥–∞–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ
            
            try:
                logger.info(f"Starting Gemini stream for user {user_id} (Attempt {attempt+1}/{MAX_RETRIES+1})")
                
                # –¢–∞–π–º–∞—É—Ç –Ω–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é —Å—Ç—Ä–∏–º–∞ (60 —Å–µ–∫—É–Ω–¥)
                STREAM_INIT_TIMEOUT = 60.0
                try:
                    stream = await asyncio.wait_for(
                        client.aio.models.generate_content_stream(**generate_kwargs),
                        timeout=STREAM_INIT_TIMEOUT
                    )
                except asyncio.TimeoutError:
                    logger.error(f"Gemini stream init timeout ({STREAM_INIT_TIMEOUT}s) for user {user_id}")
                    raise TimeoutError(f"Gemini API –Ω–µ –æ—Ç–≤–µ—Ç–∏–ª –∑–∞ {STREAM_INIT_TIMEOUT} —Å–µ–∫—É–Ω–¥")
                
                async for response in stream:
                    
                    # –°–±–æ—Ä Grounding Metadata
                    if response.candidates and response.candidates[0].grounding_metadata:
                        gm = response.candidates[0].grounding_metadata
                        if gm.grounding_chunks:
                            for chunk in gm.grounding_chunks:
                                if chunk.web and chunk.web.uri and chunk.web.title:
                                    grounding_sources[chunk.web.uri] = chunk.web.title

                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ Function Call –≤ –ø–µ—Ä–≤–æ–º —á–∞–Ω–∫–µ
                    if response.candidates and response.candidates[0].content.parts:
                        part = response.candidates[0].content.parts[0]
                        
                        if part.function_call:
                            has_started_response = True # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏ —ç—Ç–æ –æ—Ç–≤–µ—Ç
                            fc = part.function_call
                            logger.info(f"–ò–ò –≤—ã–∑—ã–≤–∞–µ—Ç —Ñ—É–Ω–∫—Ü–∏—é (STREAM): {fc.name}")
                            
                            yield f"__TOOL_CALL__:{fc.name}"
                            
                            tool_result = "–î–∞–Ω–Ω—ã–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã"
                            if fc.name == 'get_promotions':
                                now = time.time()
                                if self._promotions_cache and (now - self._promotions_cache_time < self._promotions_cache_ttl):
                                    tool_result = self._promotions_cache
                                    logger.info("Using TTLCache for promotions")
                                else:
                                    if self.promotions_gateway:
                                        try:
                                            tool_result = await get_promotions_json(self.promotions_gateway)
                                            self._promotions_cache = tool_result
                                            self._promotions_cache_time = now
                                            logger.info(f"Promotions cache updated (len: {len(tool_result)})")
                                        except Exception as te:
                                            logger.error(f"Error calling promotion tool in stream: {te}")
                                    
                            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é
                            self.user_histories[user_id].append(response.candidates[0].content)
                            function_response_part = types.Part(
                                function_response=types.FunctionResponse(
                                    name=fc.name,
                                    response={'output': tool_result}
                                )
                            )
                            self.user_histories[user_id].append(types.Content(role="tool", parts=[function_response_part]))
                            
                            # RECURSION: –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–µ–º —Å—Ç—Ä–∏–º –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ —Ñ—É–Ω–∫—Ü–∏—é
                            # –ó–¥–µ—Å—å –≤–∞–∂–Ω–æ: —Ä–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –≤—ã–∑–æ–≤ ask_stream –±—É–¥–µ—Ç –∏–º–µ—Ç—å —Å–≤–æ–π —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π —Ü–∏–∫–ª retries!
                            async for sub_part in self.ask_stream(user_id, ""): 
                                if sub_part:
                                    yield sub_part
                            return # –ü–æ–ª–Ω—ã–π –≤—ã—Ö–æ–¥ –∏–∑ —Ç–µ–∫—É—â–µ–≥–æ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ (—É—Å–ø–µ—Ö)

                        # –ï—Å–ª–∏ —ç—Ç–æ –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç
                        if response.text:
                            text_chunk = response.text
                            # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –ø—Ä–∏—à–µ–ª, –∑–Ω–∞—á–∏—Ç —ç—Ç–æ –Ω–µ –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç
                            has_started_response = True
                            full_reply_parts.append(text_chunk)
                            yield text_chunk

                # –ö–æ–Ω–µ—Ü —Ü–∏–∫–ª–∞ —Å—Ç—Ä–∏–º–∏–Ω–≥–∞ –¥–ª—è –¥–∞–Ω–Ω–æ–π –ø–æ–ø—ã—Ç–∫–∏
                
                # –ö–ª—é—á–µ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –ë—ã–ª –ª–∏ –ø–æ–ª—É—á–µ–Ω –∫–∞–∫–æ–π-—Ç–æ —Ç–µ–∫—Å—Ç?
                if not full_reply_parts:
                    # –ï—Å–ª–∏ —Å—Ç—Ä–∏–º –∑–∞–≤–µ—Ä—à–∏–ª—Å—è –±–µ–∑ —Ç–µ–∫—Å—Ç–∞ –∏ –±–µ–∑ function call -> –≠—Ç–æ "Empty Response"
                    raise ValueError("Received empty stream response from Gemini model")
                
                # –ï—Å–ª–∏ –º—ã –∑–¥–µ—Å—å, –∑–Ω–∞—á–∏—Ç –æ—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω (full_reply_parts –Ω–µ –ø—É—Å—Ç), –≤—ã—Ö–æ–¥–∏–º –∏–∑ —Ü–∏–∫–ª–∞ retry
                break 

            except Exception as e:
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–∫–∏
                if has_started_response:
                    # –ï—Å–ª–∏ –º—ã —É–∂–µ –Ω–∞—á–∞–ª–∏ —Å—Ç—Ä–∏–º–∏—Ç—å —Ç–µ–∫—Å—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é, –º—ã –ù–ï –ú–û–ñ–ï–ú –¥–µ–ª–∞—Ç—å —Ä–µ—Ç—Ä–∞–π
                    # –∏–Ω–∞—á–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —É–≤–∏–¥–∏—Ç –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–ª–∏ –∫–∞—à—É.
                    # –ü—Ä–æ—Å—Ç–æ –ª–æ–≥–∏—Ä—É–µ–º –∏ –ø—Ä–µ—Ä—ã–≤–∞–µ–º.
                    logger.error(f"Stream error AFTER yield (user {user_id}): {e}")
                    yield f"\n[‚ö†Ô∏è –û–±—Ä—ã–≤ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è: {str(e)[:50]}]"
                    return # –ü—Ä–µ—Ä—ã–≤–∞–µ–º —Å—Ç—Ä–∏–º
                
                # –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –æ—à–∏–±–∫–∏ –∏—Å—Ç–µ–∫—à–µ–≥–æ –∫—ç—à–∞ –∏–ª–∏ —É—Å—Ç–∞—Ä–µ–≤—à–µ–≥–æ API
                error_str = str(e)
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—à–∏–±–∫–∏ –∫—ç—à–∞: –∏—Å—Ç–µ–∫—à–∏–π, –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π, –∏–ª–∏ —É—Å—Ç–∞—Ä–µ–≤—à–∏–π API
                if ('CachedContent' in error_str and ('403' in error_str or 'PERMISSION_DENIED' in error_str)) or \
                   'google_search' in error_str or \
                   'not supported' in error_str.lower():
                    logger.warning(f"‚ùå Cache error or outdated API: {e}")
                    # –ò–Ω–≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞—Ç—å –∫—ç—à –≤ Knowledge Base
                    await self.knowledge_base.invalidate_cache()
                    # –ü–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å config –ë–ï–ó –∫—ç—à–∞ –¥–ª—è –ø–æ–≤—Ç–æ—Ä–∞
                    config_params['system_instruction'] = self.system_instruction
                    config_params['tools'] = tools
                    if 'cached_content' in config_params:
                        del config_params['cached_content']
                    config = types.GenerateContentConfig(**config_params)
                    generate_kwargs['config'] = config
                
                if attempt < MAX_RETRIES:
                    # Exponential Backoff: 1s, 2s, 4s...
                    wait_time = (2 ** attempt) + 0.1
                    logger.info(f"üîÑ Retrying Gemini in {wait_time}s")
                    await asyncio.sleep(wait_time) 
                    continue # –ò–¥–µ–º –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π –∫—Ä—É–≥
                else:
                    # –í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –¥–ª—è Gemini –∏—Å—á–µ—Ä–ø–∞–Ω—ã
                    logger.error(f"All {MAX_RETRIES+1} attempts failed for user {user_id}")
                    raise e

        # --- FINALIZATION (Success case) ---
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –±–ª–æ–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (Grounding)
        if grounding_sources:
            sources_text = "\n\nüìö **–ò—Å—Ç–æ—á–Ω–∏–∫–∏:**\n"
            for i, (uri, title) in enumerate(grounding_sources.items(), 1):
                sources_text += f"{i}. [{title}]({uri})\n"
            
            yield sources_text
            full_reply_parts.append(sources_text)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –≤ –∏—Å—Ç–æ—Ä–∏—é
        if full_reply_parts:
            full_reply = "".join(full_reply_parts)
            self._add_to_history(user_id, "model", full_reply)
            logger.info(f"Stream finished for user {user_id}, history updated. Sources: {len(grounding_sources)}")
            
            # –ê—Ä—Ö–∏–≤–∞—Ü–∏—è –∏—Å—Ç–æ—Ä–∏–∏ –¥–ª—è "–ø–∞–º—è—Ç–∏"
            if self.memory_archiver:
                 asyncio.create_task(self.memory_archiver.archive_user_history(
                     user_id, 
                     self.user_histories.get(user_id, [])
                 ))

    async def ask(self, user_id: int, content: str, external_history: Optional[str] = None) -> Optional[str]:
        """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∑–∞–ø—Ä–æ—Å –≤ Gemini –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç (—á–µ—Ä–µ–∑ —Å—Ç—Ä–∏–º–∏–Ω–≥)."""
        full_reply_parts = []
        async for part in self.ask_stream(user_id, content, external_history):
            full_reply_parts.append(part)
        return "".join(full_reply_parts) if full_reply_parts else None

    async def _ask_stream_openrouter(self, user_id: int, content: str, external_history: Optional[str] = None) -> AsyncGenerator[str, None]:
        """–í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –º–µ—Ç–æ–¥ –¥–ª—è —Å—Ç—Ä–∏–º–∏–Ω–≥–∞ —á–µ—Ä–µ–∑ OpenRouter."""
        MAX_OR_RETRIES = len(self.or_models)
        full_reply = ""
        current_or_model = "unknown"
        
        for attempt in range(MAX_OR_RETRIES):
            try:
                messages = []
                if self.system_instruction:
                    messages.append({"role": "system", "content": self.system_instruction})
                
                # --- Rate Limiter (Basic) ---
                # –î–∞–µ–º –ø–∞—É–∑—É –ø–µ—Ä–µ–¥ –∑–∞–ø—Ä–æ—Å–∞–º–∏ –∫ OpenRouter —á—Ç–æ–±—ã –Ω–µ –ª–æ–≤–∏—Ç—å 429
                # –û—Å–æ–±–µ–Ω–Ω–æ –≤–∞–∂–Ω–æ –¥–ª—è –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
                last_req_time = getattr(self, '_last_request_time', 0)
                time_since_last = time.time() - last_req_time
                if time_since_last < 2.0: # –ú–∏–Ω–∏–º—É–º 2 —Å–µ–∫—É–Ω–¥—ã –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                    sleep_time = 2.0 - time_since_last
                    logger.info(f"Rate limit protection: sleeping {sleep_time:.2f}s")
                    await asyncio.sleep(sleep_time)
                self._last_request_time = time.time()
                # -----------------------------

                if self.knowledge_base:
                    links = self.knowledge_base.get_file_links()
                    if links:
                        links_block = "\n### –°–°–´–õ–ö–ò –ù–ê –î–û–ö–£–ú–ï–ù–¢–´ –ë–ê–ó–´ –ó–ù–ê–ù–ò–ô:\n"
                        for fname, url in links.items():
                            links_block += f"- {fname}: {url}\n"
                        messages[0]["content"] += links_block

                if external_history and external_history.strip():
                    clean_history = external_history[-3000:]
                    messages.append({"role": "user", "content": f"–ö—Ä–∞—Ç–∫–∞—è –∏—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞:\n{clean_history}"})
                    messages.append({"role": "assistant", "content": "–ü–æ–Ω—è–ª, —É—á–∏—Ç—ã–≤–∞—é –∏—Å—Ç–æ—Ä–∏—é."})

                messages.append({"role": "user", "content": content})

                current_or_model = self.or_models[self.current_or_model_index]
                logger.info(f"OpenRouter stream for user {user_id} with model {current_or_model} (Attempt {attempt+1}/{MAX_OR_RETRIES})")

                response = await self.or_client.chat.completions.create(
                    model=current_or_model,
                    messages=messages,
                    stream=True,
                    temperature=0.7
                )

                async for chunk in response:
                    if chunk.choices[0].delta.content:
                        text = chunk.choices[0].delta.content
                        full_reply += text
                        yield text
                
                if full_reply:
                    break
                else:
                    raise ValueError("Received empty response from OpenRouter model")

            except Exception as e:
                logger.error(f"OpenRouter Error with model {current_or_model}: {e}")
                if attempt < MAX_OR_RETRIES - 1:
                    logger.warning(f"Rotating OpenRouter index and retrying (attempt {attempt+1}/{MAX_OR_RETRIES})")
                    self.current_or_model_index = (self.current_or_model_index + 1) % len(self.or_models)
                    await asyncio.sleep(1) 
                else:
                    logger.error(f"All {MAX_OR_RETRIES} OpenRouter models failed")
                    raise e 

        if self.memory_archiver and full_reply:
            fake_history = [
                types.Content(role="user", parts=[types.Part(text=content)]),
                types.Content(role="model", parts=[types.Part(text=full_reply)])
            ]
            asyncio.create_task(self.memory_archiver.archive_user_history(user_id, fake_history))

    def clear_history(self, user_id: int) -> None:
        """–û—á–∏—â–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è."""
        if user_id in self.user_histories:
            del self.user_histories[user_id]
            logger.info(f"Cleared chat history for user {user_id}")
    async def generate_image_prompt(self, text_context: str) -> Optional[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–∞ –æ—Ç–≤–µ—Ç–∞ (–ê—Ä—Ç-–¥–∏—Ä–µ–∫—Ç–æ—Ä)."""
        if not self.is_enabled():
            return None
            
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±—ã—Å—Ç—Ä—É—é –º–æ–¥–µ–ª—å-–ª–∞–π—Ç –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–æ–º–ø—Ç–∞
            model = "gemini-2.0-flash-lite-preview-02-05"
            prompt = (
                f"Analyze this text and write ONE detailed, cinematic English prompt "
                f"for high-end photorealistic image generation (8k, highly detailed) "
                f"that perfectly illustrates the context. Return ONLY the prompt.\n\n"
                f"Context: {text_context[:1000]}"
            )
            
            response = await self.client.aio.models.generate_content(
                model=model,
                contents=prompt
            )
            
            if response.text:
                logger.info(f"Image prompt generated: {response.text[:50]}...")
                return response.text.strip()
            return None
            
        except Exception as e:
            logger.error(f"Error generating image prompt: {e}")
            return None

    async def generate_image(self, prompt: str) -> Optional[bytes]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–æ –ø—Ä–æ–º–ø—Ç—É (–•—É–¥–æ–∂–Ω–∏–∫)."""
        if not self.is_enabled():
            return None
            
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º Gemini 3 Pro Image (Preview) –ø–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—é –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            # ID –∏–∑ —Å–ø–∏—Å–∫–∞ –º–æ–¥–µ–ª–µ–π: models/gemini-3-pro-image-preview
            model = "models/gemini-3-pro-image-preview"
            
            # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            config = types.GenerateImagesConfig(
                number_of_images=1,
                aspect_ratio="16:9",
                person_generation="allow_adult", # –†–∞–∑—Ä–µ—à–∞–µ–º –ª—é–¥–µ–π (–±–∏–∑–Ω–µ—Å-–∫–æ–Ω—Ç–µ–∫—Å—Ç)
                safety_filter_level="block_only_high"
            )
            
            response = await self.client.aio.models.generate_images(
                model=model,
                prompt=prompt,
                config=config
            )
            
            if response.generated_images:
                image = response.generated_images[0]
                logger.info("Image generated successfully")
                return image.image_bytes
            
            logger.warning("Models returned no images")
            return None
            
        except Exception as e:
            logger.error(f"Error generating image: {e}")
            return None
