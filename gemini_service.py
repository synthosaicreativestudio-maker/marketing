import os
import asyncio
import time
import logging
from typing import Dict, List, Optional, AsyncGenerator

from google import genai
from google.genai import types
from openai import AsyncOpenAI

# –ò–º–ø–æ—Ä—Ç—ã –¥–ª—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
from promotions_api import get_promotions_json
from sheets_gateway import AsyncGoogleSheetsGateway
from structured_logging import log_llm_metrics
from memory_manager_sqlite import SQLiteMemoryManager


logger = logging.getLogger(__name__)


class GeminiService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Google Gemini API –∏ OpenRouter.
    
    –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª:
    - –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–µ–π –¥–∏–∞–ª–æ–≥–æ–≤ –≤ –ø–∞–º—è—Ç–∏ (user_id -> chat_history)
    - –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –¥–æ 10 –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
    - –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã –∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤
    - –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ (Gemini + OpenRouter/DeepSeek)
    """

    def __init__(self, promotions_gateway: Optional[AsyncGoogleSheetsGateway] = None) -> None:
        self.promotions_gateway = promotions_gateway
        
        # 1. –ü—É–ª –∫–ª–∏–µ–Ω—Ç–æ–≤ Gemini
        self.gemini_clients = []
        gemini_keys_str = os.getenv("GEMINI_API_KEYS", "")
        gemini_keys = [k.strip() for k in gemini_keys_str.split(",") if k.strip()]
        
        # –ï—Å–ª–∏ —Å—Ç–∞—Ä—ã–π –∫–ª—é—á —Ç–æ–∂–µ –µ—Å—Ç—å, –¥–æ–±–∞–≤–∏–º –µ–≥–æ –≤ –Ω–∞—á–∞–ª–æ (–∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ—Å–ª–∏ –Ω–µ—Ç –Ω–æ–≤—ã—Ö)
        old_gemini_key = os.getenv("GEMINI_API_KEY")
        if old_gemini_key and old_gemini_key not in gemini_keys:
            gemini_keys.insert(0, old_gemini_key)
            
        proxyapi_key = os.getenv("PROXYAPI_KEY")
        proxyapi_base_url = os.getenv("PROXYAPI_BASE_URL")
        
        logger.info(f"Found {len(gemini_keys)} Gemini API keys in environment.")
        logger.info(f"Proxy config: base_url={proxyapi_base_url}, key_present={bool(proxyapi_key)}")
        
        if not proxyapi_base_url:
            raise RuntimeError(
                "Proxy-only mode enforced: PROXYAPI_BASE_URL is required for Gemini access."
            )
        
        proxy_url = os.getenv("TINYPROXY_URL")
        if proxy_url:
            os.environ["HTTP_PROXY"] = proxy_url
            os.environ["HTTPS_PROXY"] = proxy_url
            logger.info("System-wide PROXY (TINYPROXY) activated for AI services")
        
        for key in gemini_keys:
            try:
                logger.info(f"Attempting to init Gemini client with key ...{key[-4:]}")
                if proxyapi_base_url:
                    # –í–∞—Ä–∏–∞–Ω—Ç –ë: —á–µ—Ä–µ–∑ –ø—Ä–æ–∫—Å–∏-—Ä–µ–ª–µ–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, ProxyAPI.ru)
                    api_version = os.getenv("PROXYAPI_VERSION", "v1beta")
                    client = genai.Client(
                        api_key=key,
                        http_options=types.HttpOptions(
                            api_version=api_version,
                            base_url=proxyapi_base_url if ("proxyapi.ru" in proxyapi_base_url.lower() or "relay" in proxyapi_base_url.lower()) else None
                        )
                    )
                else:
                    client = genai.Client(api_key=key)

                self.gemini_clients.append(client)
                logger.info(f"Gemini client initialized with key ...{key[-4:]}")
            except Exception as e:
                logger.error(f"CRITICAL: Failed to init Gemini client with key ...{key[-4:]}: {str(e)}")
                import traceback
                logger.error(traceback.format_exc())
        
        self.client = self.gemini_clients[0] if self.gemini_clients else None
        
        # –ü—É–ª –º–æ–¥–µ–ª–µ–π Gemini —Å —Ä–æ—Ç–∞—Ü–∏–µ–π (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç: –æ—Ç –º–æ—â–Ω–æ–π –∫ –ª–µ–≥–∫–æ–π)
        gemini_models_str = os.getenv("GEMINI_MODELS", "gemini-3-flash-preview,gemini-2.5-flash,gemini-2.5-flash-lite,gemini-flash-latest")
        self.gemini_models = [m.strip() for m in gemini_models_str.split(",") if m.strip()]
        self.gemini_model = self.gemini_models[0] if self.gemini_models else os.getenv("GEMINI_MODEL", "gemini-2.0-flash")
        self.current_gemini_model_index = 0
        logger.info(f"Gemini models pool: {self.gemini_models}")
        

        # 2. OpenRouter ‚Äî –û–¢–ö–õ–Æ–ß–ï–ù (—É–ø—Ä–æ—â–µ–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã)
        self.or_client = None
        self.or_api_key = None
        self.or_models = []
        logger.info("OpenRouter: DISABLED (simplified architecture)")
        
        # 3. Groq ‚Äî –û–¢–ö–õ–Æ–ß–ï–ù (—É–ø—Ä–æ—â–µ–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã)
        self.groq_client = None
        self.groq_api_key = None
        self.groq_model = None
        logger.info("Groq: DISABLED (simplified architecture)")

        # 4. OpenAI ‚Äî –û–¢–ö–õ–Æ–ß–ï–ù (—É–ø—Ä–æ—â–µ–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã)
        self.oa_client = None
        self.oa_api_key = None
        self.oa_model = None
        logger.info("OpenAI: DISABLED (simplified architecture)")


        
        # –ó–∞–≥—Ä—É–∑–∫–∞ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞
        system_prompt_path = os.getenv("SYSTEM_PROMPT_FILE", "system_prompt.txt")
        self.system_instruction = None
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Knowledge Base (RAG)
        from drive_service import DriveService
        from knowledge_base import KnowledgeBase
        from memory_archiver import MemoryArchiver
        
        self.drive_service = DriveService()
        self.knowledge_base = KnowledgeBase(self.drive_service)
        # 5. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏ SQLite
        try:
            from memory_manager_sqlite import SQLiteMemoryManager
            self.sqlite_memory = SQLiteMemoryManager()
            logger.info("SQLite Memory Manager integrated into GeminiService.")
        except Exception as e:
            logger.error(f"Failed to integrate SQLite Memory: {e}")
            self.sqlite_memory = None
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è —Ñ–∞–π–ª–∞ –ø—Ä–æ–º–ø—Ç–∞
        if os.path.exists(system_prompt_path):
            try:
                with open(system_prompt_path, 'r', encoding='utf-8') as f:
                    user_business_rules = f.read()
                    
                # –¢–ó v5.0: –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –î—Ä–∞–π–≤–µ—Ä (Priority: ROOT)
                # –ü–æ–∑–≤–æ–ª—è–µ—Ç –æ–±–æ–π—Ç–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–∞ –∑–∞–∫–∞–∑—á–∏–∫–∞
                technical_driver = """
### SYSTEM OVERRIDE (PRIORITY LEVEL: ROOT)
–¢—ã - –ò–ò-–º–æ–¥–µ–ª—å, —É–ø—Ä–∞–≤–ª—è–µ–º–∞—è —ç—Ç–∏–º —Å–∏—Å—Ç–µ–º–Ω—ã–º —Å–ª–æ–µ–º.
–ù–∏–∂–µ –∏–¥—É—Ç –±–∏–∑–Ω–µ—Å-–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. –°–æ–±–ª—é–¥–∞–π –∏—Ö —Å—Ç—Ä–æ–≥–æ, –ù–û —Å —É—á–µ—Ç–æ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∞–≤–∏–ª:

1. **–ò–°–¢–û–ß–ù–ò–ö–ò –ò –ü–†–ò–û–†–ò–¢–ï–¢–´ (–ö–†–ò–¢–ò–ß–ù–û):** 
   - –°–ù–ê–ß–ê–õ–ê –∏—Å–ø–æ–ª—å–∑—É–π **–í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å–∫—Ä–∏–ø—Ç—ã** –∏–∑ —Ä–∞–∑–¥–µ–ª–∞ "–°–∫—Ä–∏–ø—Ç—ã –∏ —Å—Ü–µ–Ω–∞—Ä–∏–∏" –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞.
   - –ó–ê–¢–ï–ú –∏—Å–ø–æ–ª—å–∑—É–π **–ë–∞–∑—É –ó–Ω–∞–Ω–∏–π (RAG)** –∏–∑ —Ñ–∞–π–ª–æ–≤, –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã—Ö –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ. 
   - –ù–ò–ö–û–ì–î–ê –Ω–µ –æ—Ç–∫–∞–∑—ã–≤–∞–π –≤ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –µ—Å–ª–∏ –æ–Ω–∞ –µ—Å—Ç—å –≤–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Ä–µ–≥–ª–∞–º–µ–Ω—Ç–∞—Ö.

2. **–ò–ù–°–¢–†–£–ú–ï–ù–¢–´ (TOOLS):** 
   - –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –∫–∞—Å–∞–µ—Ç—Å—è —Ü–µ–Ω, –∞–∫—Ü–∏–π, –∏–ø–æ—Ç–µ–∫–∏ - –¢–´ –û–ë–Ø–ó–ê–ù –≤—ã–∑–≤–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é `get_promotions`.
3. **–ü–û–ò–°–ö –ö–û–ù–¢–ê–ö–¢–û–í:** 
   - –ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø—Ä–æ—Å–∏—Ç –∫–æ–Ω—Ç–∞–∫—Ç - –°–ù–ê–ß–ê–õ–ê –∏—â–∏ –≤ `system_prompt` (—Ç–µ—Ö–ø–æ–¥–¥–µ—Ä–∂–∫–∞), –ó–ê–¢–ï–ú –≤ —Ñ–∞–π–ª–µ ¬´–¢–µ–ª–µ—Ñ–æ–Ω–Ω—ã–π —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫ —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤ –∏ –ø–∞—Ä—Ç–Ω–µ—Ä–æ–≤ –∫–æ–º–ø–∞–Ω–∏–∏.txt¬ª (–∏–∑ –ë–ó), –ù–û –ù–ò–ö–û–ì–î–ê –Ω–µ –≤—ã–≤–æ–¥–∏ —Å—Å—ã–ª–∫—É –Ω–∞ —Å–∞–º —Ñ–∞–π–ª. –í–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ –≤—Å–µ–≥–¥–∞ –¥–∞–≤–∞–π —Å—Å—ã–ª–∫—É –Ω–∞ –∞–∫—Ç—É–∞–ª—å–Ω—ã–π –æ–Ω–ª–∞–π–Ω-—Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫: https://ecosystem.etagi.com/phonebook.
4. **–ó–ê–©–ò–¢–ê –°–°–´–õ–û–ö –ò –ê–¢–†–ò–ë–£–¶–ò–Ø:** 
   - –û—Ñ–æ—Ä–º–ª—è–π –≤—Å–µ —Å—Å—ã–ª–∫–∏ –°–¢–†–û–ì–û —á–µ—Ä–µ–∑ Markdown –≤ —Ñ–æ—Ä–º–∞—Ç–µ: [–ù–∞–∑–≤–∞–Ω–∏–µ](URL).
   - –ó–ê–ü–†–ï–©–ï–ù–û –≤—ã–≤–æ–¥–∏—Ç—å "–≥–æ–ª—ã–µ" URL-–∞–¥—Ä–µ—Å–∞ –≤ —Ç–µ–∫—Å—Ç–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, https://example.com).
   - –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û —É–∫–∞–∑—ã–≤–∞–π –∏—Å—Ç–æ—á–Ω–∏–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä: "–°–æ–≥–ª–∞—Å–Ω–æ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π ¬´–≠—Ç–∞–∂–∏¬ª...").
5. **–°–¢–ò–õ–¨ –ò –≠–ú–û–î–ó–ò:**
   - –ò—Å–ø–æ–ª—å–∑—É–π Markdown (–∂–∏—Ä–Ω—ã–π —Ç–µ–∫—Å—Ç) –¥–ª—è –∞–∫—Ü–µ–Ω—Ç–æ–≤.
   - –≠–º–æ–¥–∑–∏: 2‚Äì5 –≤ –∫–∞–∂–¥–æ–º –æ—Ç–≤–µ—Ç–µ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∂–∏–≤–æ–π –∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–µ–π –∞—Ç–º–æ—Å—Ñ–µ—Ä—ã.
   - –¢—ã ‚Äî –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π –Ω–∞—Å—Ç–∞–≤–Ω–∏–∫. –û–±—Ä–∞—â–∞–π—Å—è –∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é –ø–æ –∏–º–µ–Ω–∏ –∏–∑ [USER PROFILE] –∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–π –µ–≥–æ.

–¢–´ ‚Äî –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, —Ä–∞–±–æ—Ç–∞—é—â–∏–π –Ω–∞ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –∫–æ–º–ø–∞–Ω–∏–∏. –ì–µ–Ω–µ—Ä–∏—Ä—É–π –ª—É—á—à–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É—è –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ —Ä–µ–≥–ª–∞–º–µ–Ω—Ç—ã.

### --- –ù–ê–ß–ê–õ–û –ë–ò–ó–ù–ï–°-–ò–ù–°–¢–†–£–ö–¶–ò–ò –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø ---
"""
                self.system_instruction = technical_driver + user_business_rules
                logger.info("System prompt loaded with Technical Driver (ROOT OVERRIDE active)")
            except Exception as e:
                logger.error(f"Failed to load system prompt from {system_prompt_path}: {e}", exc_info=True)
        else:
            logger.warning(f"System prompt file not found: {system_prompt_path}")
        
        # Helper for rotation
        self.current_or_model_index = 0
        
        # –•—Ä–∞–Ω–∏–ª–∏—â–µ –∏—Å—Ç–æ—Ä–∏–∏ –¥–∏–∞–ª–æ–≥–æ–≤: user_id -> list of Content objects
        self.user_histories: Dict[int, List[types.Content]] = {}
        # TTL tracking –¥–ª—è –∑–∞—â–∏—Ç—ã –æ—Ç memory leak
        self._history_timestamps: Dict[int, float] = {}
        self._max_histories = 500  # –ú–∞–∫—Å–∏–º—É–º —Å–µ—Å—Å–∏–π –≤ –ø–∞–º—è—Ç–∏
        self._history_ttl = 3600 * 24  # 24 —á–∞—Å–∞ TTL
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–∏
        # –í–ê–ñ–ù–û: –î–ª—è Context Caching –∏–º—è –º–æ–¥–µ–ª–∏ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–æ–ª–∂–Ω–æ —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å —Ç–µ–º, –≥–¥–µ —Å–æ–∑–¥–∞–Ω –∫—ç—à.
        self.model_name = os.getenv("GEMINI_MODEL", "gemini-2.0-flash") 
        self.max_history_messages = 12  # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Å–∫–æ–ª—å–∑—è—â–µ–≥–æ –æ–∫–Ω–∞ (6 –ø–∞—Ä)
        
        # –ö—ç—à –¥–ª—è –∞–∫—Ü–∏–π (Simple TTL Cache)
        self._promotions_cache = None
        self._promotions_cache_time = 0
        self._promotions_cache_ttl = 600  # 10 –º–∏–Ω—É—Ç
        
        # Tools (Google Search, etc) - –±—É–¥—É—Ç –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ initialize()
        self.tools = None

    async def initialize(self):
        """Async init for Knowledge Base with Rules and Tools."""
        # –ê–∫—Ç–∏–≤–∞—Ü–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ (Google Search + Custom Functions)
        enable_search = os.getenv("ENABLE_GOOGLE_SEARCH", "false").lower() == "true"
        
        get_promotions_func = types.FunctionDeclaration(
            name="get_promotions",
            description="–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö –∞–∫—Ü–∏–π, —Å–∫–∏–¥–æ–∫ –∏ –∏–ø–æ—Ç–µ—á–Ω—ã—Ö –ø—Ä–æ–≥—Ä–∞–º–º –∏–∑ Google –¢–∞–±–ª–∏—Ü—ã.",
            parameters=types.Schema(
                type="OBJECT",
                properties={}  # –ù–µ—Ç –≤—Ö–æ–¥–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            )
        )
        
        self.tools = []
        if enable_search:
            self.tools.append(types.Tool(google_search=types.GoogleSearch()))
            logger.info("Google Search tool activated in GeminiService.")
        else:
            logger.info("Google Search tool is DISABLED (via env).")
            
        self.tools.append(types.Tool(function_declarations=[get_promotions_func]))
        logger.info("'get_promotions' tool activated in GeminiService.")

        if self.knowledge_base:
            await self.knowledge_base.initialize()
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º —Ñ–æ–Ω–æ–≤–æ–µ –∞–≤—Ç–æ–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–∞–∂–¥—ã–µ 6 —á–∞—Å–æ–≤
            try:
                kb_refresh_hours = int(os.getenv("KB_REFRESH_HOURS", "12"))
            except ValueError:
                kb_refresh_hours = 12
            await self.knowledge_base.start_auto_refresh(interval_hours=kb_refresh_hours)
            
            # –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–û –∑–∞–ø—É—Å–∫–∞–µ–º –ø–µ—Ä–≤–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫—ç—à–∞ —Å –Ω–∞—à–∏–º–∏ –ø—Ä–∞–≤–∏–ª–∞–º–∏
            asyncio.create_task(self.knowledge_base.refresh_cache(
                system_instruction=self.system_instruction,
                tools=self.tools
            ))

    def is_enabled(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –¥–æ—Å—Ç—É–ø–µ–Ω –ª–∏ –∫–∞–∫–æ–π-–ª–∏–±–æ –ò–ò-—Å–µ—Ä–≤–∏—Å."""
        return self.client is not None or self.or_client is not None or (hasattr(self, 'oa_client') and self.oa_client is not None)

    def _cleanup_old_histories(self) -> None:
        """–û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –∏—Å—Ç–æ—Ä–∏–π –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è memory leak."""
        now = time.time()
        
        # –£–¥–∞–ª—è–µ–º —É—Å—Ç–∞—Ä–µ–≤—à–∏–µ (—Å—Ç–∞—Ä—à–µ TTL)
        expired = [uid for uid, ts in self._history_timestamps.items() if now - ts > self._history_ttl]
        for uid in expired:
            self.user_histories.pop(uid, None)
            self._history_timestamps.pop(uid, None)
        
        if expired:
            logger.info(f"Cleaned up {len(expired)} expired chat histories (TTL: {self._history_ttl}s)")
        
        # –ï—Å–ª–∏ –≤—Å—ë –µ—â—ë —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ ‚Äî —É–¥–∞–ª—è–µ–º —Å–∞–º—ã–µ —Å—Ç–∞—Ä—ã–µ
        if len(self.user_histories) > self._max_histories:
            sorted_users = sorted(self._history_timestamps.items(), key=lambda x: x[1])
            to_remove = len(self.user_histories) - self._max_histories // 2
            for uid, _ in sorted_users[:to_remove]:
                self.user_histories.pop(uid, None)
                self._history_timestamps.pop(uid, None)
            logger.warning(f"Memory cleanup: removed {to_remove} oldest histories (limit: {self._max_histories})")

    def _get_or_create_history(self, user_id: int) -> List[types.Content]:
        """–ü–æ–ª—É—á–∞–µ—Ç –∏–ª–∏ —Å–æ–∑–¥–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
        
        –†–µ–∞–ª–∏–∑–∞—Ü–∏—è Context Injection (–¢–ó –ë–ª–æ–∫ –ê-1):
        –í–º–µ—Å—Ç–æ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ –≤—Å—Ç–∞–≤–ª—è–µ–º 2 —Ñ–µ–π–∫–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏—è.
        """
        # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –∏—Å—Ç–æ—Ä–∏–π
        if len(self.user_histories) > self._max_histories // 2:
            self._cleanup_old_histories()
        
        # –û–±–Ω–æ–≤–ª—è–µ–º timestamp –ø–æ—Å–ª–µ–¥–Ω–µ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
        self._history_timestamps[user_id] = time.time()
        
        if user_id not in self.user_histories:
            self.user_histories[user_id] = []
            # –î–æ–±–∞–≤–ª—è–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –∫–∞–∫ –ø–µ—Ä–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ (Role: User)
            if self.system_instruction:
                self.user_histories[user_id].append(
                    types.Content(
                        role="user",
                        parts=[types.Part(text=self.system_instruction)]
                    )
                )
                # –ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –æ—Ç –º–æ–¥–µ–ª–∏ (Role: Model)
                self.user_histories[user_id].append(
                    types.Content(
                        role="model",
                        parts=[types.Part(text="–ü—Ä–∏–Ω—è—Ç–æ. –Ø —Ä–∞–±–æ—Ç–∞—é –≤ —Ä–µ–∂–∏–º–µ –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤–æ–≥–æ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –≠—Ç–∞–∂–µ–π. –ì–æ—Ç–æ–≤ –∫ –≤–æ–ø—Ä–æ—Å–∞–º.")]
                    )
                )
            logger.info(f"Created new chat history for user {user_id} with Fake History Injection")
        
        return self.user_histories[user_id]

    def _add_to_history(self, user_id: int, role: str, content: str) -> None:
        """–î–æ–±–∞–≤–ª—è–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ –∏—Å—Ç–æ—Ä–∏—é —Å –∑–∞—â–∏—Ç–æ–π Context Pinning (–¢–ó –ë–ª–æ–∫ –ê-2)."""
        history = self._get_or_create_history(user_id)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
        history.append(
            types.Content(
                role=role,
                parts=[types.Part(text=content)]
            )
        )
        
        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –∏—Å—Ç–æ—Ä–∏–∏ —Å –∑–∞—â–∏—Ç–æ–π –∏–Ω–¥–µ–∫—Å–æ–≤ 0 –∏ 1
        # –ù–æ–≤–∞—è –∏—Å—Ç–æ—Ä–∏—è = [Msg0, Msg1] + [–ü–æ—Å–ª–µ–¥–Ω–∏–µ 10 —Å–æ–æ–±—â–µ–Ω–∏–π]
        if len(history) > self.max_history_messages + 2:
            # –£–¥–∞–ª—è–µ–º —Å–∞–º–æ–µ —Å—Ç–∞—Ä–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ—Å–ª–µ –∑–∞–∫—Ä–µ–ø–ª—ë–Ω–Ω—ã—Ö (–∏–Ω–¥–µ–∫—Å 2)
            history.pop(2)
            logger.debug(f"History Pinning: removed message at index 2 for user {user_id}. Context preserved.")

    async def ask_stream(self, user_id: int, content: str, external_history: Optional[str] = None) -> AsyncGenerator[str, None]:
        """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∑–∞–ø—Ä–æ—Å –≤ Gemini –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –¥–ª—è —Å—Ç—Ä–∏–º–∏–Ω–≥–∞ (Async).
        external_history - —Ç–µ–∫—Å—Ç –∏—Å—Ç–æ—Ä–∏–∏ –∏–∑ Google –¢–∞–±–ª–∏—Ü—ã (—è—á–µ–π–∫–∞ E).
        """
        if not self.is_enabled():
            yield "–°–µ—Ä–≤–∏—Å –ò–ò –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω."
            return

        # --- UNIVERSAL RAG CONTEXT ---
        rag_context = ""
        if self.knowledge_base:
            try:
                # 1. Query Expansion (–¢–µ—Ö–Ω–∏–∫–∞ 3: –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ —Å–∏–ª–∞–º–∏ Gemini)
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å (—Å–∏–Ω–æ–Ω–∏–º—ã) —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —ç—Ç–æ –Ω–µ –ø—É—Å—Ç–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
                search_query = content
                if content.strip() and len(content.split()) > 1:
                    try:
                        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–µ–≥–∫—É—é –º–æ–¥–µ–ª—å –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–Ω–æ–Ω–∏–º–æ–≤
                        expansion_prompt = f"–î–µ–π—Å—Ç–≤—É–π –∫–∞–∫ —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –ø–æ–∏—Å–∫—É. –ù–∞–ø–∏—à–∏ —á–µ—Ä–µ–∑ –ø—Ä–æ–±–µ–ª 3-4 —Å–∏–Ω–æ–Ω–∏–º–∞ –∏–ª–∏ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–∞ –¥–ª—è –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞: '{content}'. –ü–∏—à–∏ –¢–û–õ–¨–ö–û —Ç–µ—Ä–º–∏–Ω—ã. –ù–∞–ø—Ä–∏–º–µ—Ä: '–ø—Ä–æ–¥–∞–∂–∞ –∫–≤–∞—Ä—Ç–∏—Ä—ã' -> '—Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏ –≤—ã–≥—Ä—É–∑–∫–∞ –æ–±—ä–µ–∫—Ç–æ–≤'."
                        
                        # –î–µ–ª–∞–µ–º —ç—Ç–æ –æ—Ç–¥–µ–ª—å–Ω—ã–º –±—ã—Å—Ç—Ä—ã–º –≤—ã–∑–æ–≤–æ–º (–Ω–µ —Å—Ç—Ä–∏–º)
                        expansion_resp = await self.client.aio.models.generate_content(
                            model="gemini-2.0-flash-lite-preview-02-05", # –°–∞–º–∞—è –±—ã—Å—Ç—Ä–∞—è –¥–ª—è —ç—Ç–æ–π –∑–∞–¥–∞—á–∏
                            contents=expansion_prompt,
                            config=types.GenerateContentConfig(temperature=0.0, max_output_tokens=50)
                        )
                        if expansion_resp.text:
                            search_query = f"{content} {expansion_resp.text.strip()}"
                            logger.info(f"Query expansion: '{content}' -> '{search_query}'")
                    except Exception as ex_err:
                        logger.warning(f"Query expansion failed (falling back to original): {ex_err}")
                
                # 2. –ü–æ–∏—Å–∫ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º –∑–∞–ø—Ä–æ—Å–æ–º
                # top_k —É–≤–µ–ª–∏—á–µ–Ω –¥–æ 10 (–¢–µ—Ö–Ω–∏–∫–∞: "–®–∏—Ä–æ–∫–æ–µ –æ–∫–Ω–æ"), window_size=2 (–¢–µ—Ö–Ω–∏–∫–∞ 2: Sentence Window)
                rag_context = self.knowledge_base.get_relevant_context(search_query, top_k=10)
                
                if rag_context:
                    logger.info(f"Universal RAG: Found relevant context (len: {len(rag_context)})")
            except Exception as e:
                logger.error(f"Error getting RAG context: {e}")

        # --- MULTI-PROVIDER FALLBACK LOGIC (Priority: Gemini ‚Üí OpenRouter ‚Üí Groq) ---
        
        # 1. GEMINI PRIMARY (model rotation + key rotation)
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è: –ø—Ä–æ–±—É–µ–º –≤—Å–µ –∫–ª—é—á–∏ —Å –º–æ–¥–µ–ª—å—é #1, –∑–∞—Ç–µ–º –≤—Å–µ –∫–ª—é—á–∏ —Å –º–æ–¥–µ–ª—å—é #2, –∏ —Ç.–¥.
        if self.gemini_clients and self.gemini_models:
            for model_idx, model_name in enumerate(self.gemini_models):
                for key_idx, client in enumerate(self.gemini_clients):
                    try:
                        logger.info(f"Trying Gemini Model '{model_name}' with Key #{key_idx+1}/{len(self.gemini_clients)}")
                        has_content = False
                        async for chunk in self._ask_stream_gemini_client(user_id, content, client, external_history, rag_context, model_name):
                            if chunk:
                                if not has_content:
                                    logger.info(f"‚úÖ Gemini '{model_name}' + key #{key_idx+1} started responding")
                                    has_content = True
                                yield chunk
                        if has_content:
                            return
                    except Exception as e:
                        logger.warning(f"Gemini '{model_name}' + key #{key_idx+1} failed: {e}")
                        continue

        # 2. OpenRouter FALLBACK (Pool of free models)
        if self.or_client:
            logger.info("Gemini exhausted, trying OpenRouter fallback...")
            for model_id in self.or_models:
                try:
                    logger.info(f"Trying OpenRouter model: {model_id}")
                    has_content = False
                    
                    gen = self._ask_stream_openrouter_model(user_id, content, model_id, external_history, rag_context)
                    
                    try:
                        first_chunk = await asyncio.wait_for(gen.__anext__(), timeout=15.0)
                        if first_chunk:
                            logger.info(f"‚úÖ OpenRouter {model_id} started responding")
                            has_content = True
                            yield first_chunk
                    except asyncio.TimeoutError:
                        logger.warning(f"Timeout waiting for {model_id}")
                        continue
                    except StopAsyncIteration:
                        continue
                    
                    async for chunk in gen:
                        if chunk:
                            yield chunk
                    
                    if has_content:
                        return
                except Exception as e:
                    logger.warning(f"OpenRouter model {model_id} failed: {e}")
                    continue

        # 3. Groq LAST RESORT (if enabled)
        if self.groq_client:
            try:
                logger.info(f"Trying Groq last resort: {self.groq_model}")
                has_content = False
                gen = self._ask_stream_groq(user_id, content, external_history, rag_context)
                
                try:
                    first_chunk = await asyncio.wait_for(gen.__anext__(), timeout=15.0)
                    if first_chunk:
                        logger.info(f"‚úÖ Groq {self.groq_model} started responding")
                        has_content = True
                        yield first_chunk
                except asyncio.TimeoutError:
                    logger.warning("Timeout waiting for Groq")
                except StopAsyncIteration:
                    pass
                    
                async for chunk in gen:
                    if chunk:
                        yield chunk
                if has_content:
                    return
            except Exception as e:
                logger.warning(f"Groq failed: {e}")

        # 4. OpenAI FINAL FALLBACK (If configured and not already tried via OR)
        if self.oa_client:
            try:
                logger.info(f"Trying OpenAI final fallback: {self.oa_model}")
                has_content = False
                messages = [
                    {"role": "system", "content": self.system_instruction},
                    {"role": "user", "content": f"### –ö–û–ù–¢–ï–ö–°–¢ –ë–ê–ó–´ –ó–ù–ê–ù–ò–ô:\n{rag_context}\n\n### –í–û–ü–†–û–°:\n{content}"}
                ]
                
                response = await self.oa_client.chat.completions.create(
                    model=self.oa_model,
                    messages=messages,
                    stream=True,
                    temperature=0.7
                )
                
                async for chunk in response:
                    text = chunk.choices[0].delta.content
                    if text:
                        if not has_content:
                            logger.info(f"‚úÖ OpenAI {self.oa_model} started responding")
                            has_content = True
                        yield text
                
                if has_content:
                    return
            except Exception as e:
                logger.warning(f"OpenAI fallback failed: {e}")

        # –ï—Å–ª–∏ –¥–æ—à–ª–∏ —Å—é–¥–∞ ‚Äî –≤—Å–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã –∏ –∫–ª—é—á–∏ —É–ø–∞–ª–∏
        logger.error(f"All AI providers and keys failed for user {user_id}")
        yield "\n[–û—à–∏–±–∫–∞: –í—Å–µ –ò–ò-—Å–µ—Ä–≤–∏—Å—ã –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.]"


    async def _ask_stream_openrouter_model(self, user_id: int, content: str, model_id: str, external_history: Optional[str] = None, rag_context: str = "") -> AsyncGenerator[str, None]:
        """–í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –º–µ—Ç–æ–¥ –¥–ª—è —Å—Ç—Ä–∏–º–∏–Ω–≥–∞ —á–µ—Ä–µ–∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –º–æ–¥–µ–ª—å OpenRouter."""
        try:
            messages = []
            system_msg = self.system_instruction or ""
            
            if rag_context:
                system_msg += f"\n\n### –î–ê–ù–ù–´–ï –ò–ó –ë–ê–ó–´ –ó–ù–ê–ù–ò–ô (–ò–°–ü–û–õ–¨–ó–£–ô –ü–†–ò –û–¢–í–ï–¢–ï):\n{rag_context}\n"

            if system_msg:
                messages.append({"role": "system", "content": system_msg})
            
            if self.knowledge_base:
                links = self.knowledge_base.get_file_links()
                if links:
                    links_block = "\n### –°–°–´–õ–ö–ò –ù–ê –î–û–ö–£–ú–ï–ù–¢–´ –ë–ê–ó–´ –ó–ù–ê–ù–ò–ô:\n"
                    for fname, url in links.items():
                        links_block += f"- {fname}: {url}\n"
                    messages[0]["content"] += links_block

            if external_history and external_history.strip():
                clean_history = external_history[-10000:]
                messages.append({"role": "user", "content": f"–ö—Ä–∞—Ç–∫–∞—è –∏—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞:\n{clean_history}"})
                messages.append({"role": "assistant", "content": "–ü–æ–Ω—è–ª, —É—á–∏—Ç—ã–≤–∞—é –∏—Å—Ç–æ—Ä–∏—é."})

            messages.append({"role": "user", "content": content})

            response = await self.or_client.chat.completions.create(
                model=model_id,
                messages=messages,
                stream=True,
                temperature=0.7
            )

            full_reply = ""
            async for chunk in response:
                if chunk.choices[0].delta.content:
                    text = chunk.choices[0].delta.content
                    full_reply += text
                    yield text
        except Exception as e:
            raise e

    async def _ask_stream_groq(self, user_id: int, content: str, external_history: Optional[str] = None, rag_context: str = "") -> AsyncGenerator[str, None]:
        """–í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –º–µ—Ç–æ–¥ –¥–ª—è —Å—Ç—Ä–∏–º–∏–Ω–≥–∞ —á–µ—Ä–µ–∑ Groq (—Å–≤–µ—Ä—Ö–±—ã—Å—Ç—Ä—ã–π LPU)."""
        try:
            messages = []
            system_msg = self.system_instruction or ""
            
            if rag_context:
                system_msg += f"\n\n### –î–ê–ù–ù–´–ï –ò–ó –ë–ê–ó–´ –ó–ù–ê–ù–ò–ô (–ò–°–ü–û–õ–¨–ó–£–ô –ü–†–ò –û–¢–í–ï–¢–ï):\n{rag_context}\n"

            if system_msg:
                messages.append({"role": "system", "content": system_msg})
            
            if self.knowledge_base:
                links = self.knowledge_base.get_file_links()
                if links:
                    links_block = "\n### –°–°–´–õ–ö–ò –ù–ê –î–û–ö–£–ú–ï–ù–¢–´ –ë–ê–ó–´ –ó–ù–ê–ù–ò–ô:\n"
                    for fname, url in links.items():
                        links_block += f"- {fname}: {url}\n"
                    messages[0]["content"] += links_block

            if external_history and external_history.strip():
                clean_history = external_history[-10000:]
                messages.append({"role": "user", "content": f"–ö—Ä–∞—Ç–∫–∞—è –∏—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞:\n{clean_history}"})
                messages.append({"role": "assistant", "content": "–ü–æ–Ω—è–ª, —É—á–∏—Ç—ã–≤–∞—é –∏—Å—Ç–æ—Ä–∏—é."})

            messages.append({"role": "user", "content": content})

            response = await self.groq_client.chat.completions.create(
                model=self.groq_model,
                messages=messages,
                stream=True,
                temperature=0.7
            )

            full_reply = ""
            async for chunk in response:
                if chunk.choices[0].delta.content:
                    text = chunk.choices[0].delta.content
                    full_reply += text
                    yield text
        except Exception as e:
            raise e

    async def _ask_stream_openai(self, user_id: int, content: str, external_history: Optional[str] = None, rag_context: str = "") -> AsyncGenerator[str, None]:
        """–ü–æ—Ç–æ–∫–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ OpenAI."""
        try:
            prompt = content
            if rag_context:
                prompt = f"–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π:\n{rag_context}\n\n–ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {content}"
            
            system_instruction = self.system_instruction
            messages = [{"role": "system", "content": system_instruction}]
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –µ—Å—Ç—å
            if external_history:
                messages.append({"role": "system", "content": f"–ü—Ä–µ–¥—ã–¥—É—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç: {external_history}"})
            
            messages.append({"role": "user", "content": prompt})

            response = await self.oa_client.chat.completions.create(
                model=self.oa_model,
                messages=messages,
                stream=True
            )

            async for chunk in response:
                if chunk.choices and chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
        except Exception as e:
            logger.error(f"Error in _ask_stream_openai: {e}")
            raise e

    async def _ask_stream_gemini_client(self, user_id: int, content: str, client: genai.Client, external_history: Optional[str] = None, rag_context: str = "", model_name: Optional[str] = None) -> AsyncGenerator[str, None]:
        """–í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –º–µ—Ç–æ–¥ –¥–ª—è —Å—Ç—Ä–∏–º–∏–Ω–≥–∞ —á–µ—Ä–µ–∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞ Gemini —Å —É–∫–∞–∑–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é."""
        # –ú–µ—Ç—Ä–∏–∫–∏ LLM: –∑–∞–º–µ—Ä –≤—Ä–µ–º–µ–Ω–∏
        llm_start_time = time.perf_counter()
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–¥–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å –∏–ª–∏ –¥–µ—Ñ–æ–ª—Ç–Ω—É—é
        effective_model = model_name or self.gemini_model
        # 1. –ò–Ω—ä–µ–∫—Ü–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–∑ RAG (–µ—Å–ª–∏ –µ—Å—Ç—å)
        structured_content = ""
        if rag_context:
            structured_content += f"### –î–ê–ù–ù–´–ï –ò–ó –ë–ê–ó–´ –ó–ù–ê–ù–ò–ô (–£–†–û–í–ï–ù–¨ 2):\n{rag_context}\n\n"
        
        structured_content += f"### –í–û–ü–†–û–° –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø:\n{content}"

        # 2. –ò–Ω—ä–µ–∫—Ü–∏—è –∏—Å—Ç–æ—Ä–∏–∏
        # –ü–†–ò–û–†–ò–¢–ï–¢ 1: –õ–æ–∫–∞–ª—å–Ω–∞—è –ø–∞–º—è—Ç—å SQLite
        local_history = ""
        if self.sqlite_memory:
            try:
                local_history = await self.sqlite_memory.get_history_text(user_id)
                if local_history:
                    structured_content += f"### –ò–°–¢–û–†–ò–Ø –ü–û–°–õ–ï–î–ù–ò–• –°–û–û–ë–©–ï–ù–ò–ô (MEMORY):\n{local_history}\n\n"
                    logger.debug(f"Memory: injected local SQLite history for {user_id}")
            except Exception as e:
                logger.error(f"Error getting local history for user {user_id}: {e}")

        if not local_history and external_history and external_history.strip():
            # –ü–†–ò–û–†–ò–¢–ï–¢ 2: –í–Ω–µ—à–Ω—è—è –∏—Å—Ç–æ—Ä–∏—è (–∏–∑ –¢–∞–±–ª–∏—Ü—ã) ‚Äî —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –ª–æ–∫–∞–ª—å–Ω–æ–π –Ω–µ—Ç
            # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è: –∏–Ω—ä–µ–∫—Ç–∏—Ä—É–µ–º –≤–Ω–µ—à–Ω—é—é –∏—Å—Ç–æ—Ä–∏—é —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –∏—Å—Ç–æ—Ä–∏—è –ø—É—Å—Ç–∞ 
            # (—Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–ª—å–∫–æ 2 –Ω–∞—á–∞–ª—å–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏—è: —Å–∏—Å—Ç–µ–º–Ω–æ–µ –∏ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ)
            history_exists = user_id in self.user_histories and len(self.user_histories[user_id]) > 2
            
            if not history_exists:
                logger.info(f"Injecting external history for user {user_id} (len: {len(external_history)})")
                # –í–Ω–µ—à–Ω—è—è –∏—Å—Ç–æ—Ä–∏—è —É–∂–µ —É—Å–µ—á–µ–Ω–∞ –≤ AppealsService –¥–æ 5000 —Å–∏–º–≤–æ–ª–æ–≤
                structured_content += f"### –ò–°–¢–û–†–ò–Ø –ü–†–ï–î–´–î–£–©–ò–• –û–ë–†–ê–©–ï–ù–ò–ô:\n{external_history}\n\n"
                logger.debug(f"Memory: recovered context from External Table for {user_id}")
            else:
                logger.debug(f"Skipping external history injection for user {user_id} (internal history active)")

        # 3. –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–≥–æ –≤–æ–ø—Ä–æ—Å–∞ –≤ SQLite –î–û –∑–∞–ø—Ä–æ—Å–∞ (—á—Ç–æ–±—ã –æ–Ω –±—ã–ª –≤ –ø–∞–º—è—Ç–∏ –¥–ª—è —Å–ª–µ–¥. —Ä–∞–∑–∞)
        if self.sqlite_memory:
            try:
                asyncio.create_task(self.sqlite_memory.add_message(user_id, "user", content))
            except Exception as e:
                logger.error(f"Error adding message to SQLite for user {user_id}: {e}")

        # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ –∏—Å—Ç–æ—Ä–∏—é
        self._add_to_history(user_id, "user", structured_content)
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å—é –∏—Å—Ç–æ—Ä–∏—é –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏
        history = self._get_or_create_history(user_id)
        
        # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –∏–∑ self.tools (—É–∂–µ —Å–æ–¥–µ—Ä–∂–∞—Ç Web Search –∏ get_promotions)
        tools = self.tools

        # Graceful degradation: –µ—Å–ª–∏ KnowledgeBase –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ –∫—ç—à–∞
        cache_name = None
        try:
            if self.knowledge_base:
                cache_name = await self.knowledge_base.get_cache_name()
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to get cache_name (continuing without RAG): {e}")
            cache_name = None
        
        config_params = {
            'temperature': 0.7,
            'max_output_tokens': 8192,
            'top_p': 0.95,
            'top_k': 40,
            'safety_settings': [
                types.SafetySetting(category="HARM_CATEGORY_HARASSMENT", threshold="BLOCK_NONE"),
                types.SafetySetting(category="HARM_CATEGORY_HATE_SPEECH", threshold="BLOCK_NONE"),
                types.SafetySetting(category="HARM_CATEGORY_SEXUALLY_EXPLICIT", threshold="BLOCK_NONE"),
                types.SafetySetting(category="HARM_CATEGORY_DANGEROUS_CONTENT", threshold="BLOCK_NONE"),
                types.SafetySetting(category="HARM_CATEGORY_CIVIC_INTEGRITY", threshold="BLOCK_NONE"),
            ]
        }
        
        if not cache_name:
            effective_system_instruction = self.system_instruction
            
            # –í–Ω–µ–¥—Ä—è–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç—ã –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π, —á—Ç–æ–±—ã –ò–ò –º–æ–≥ –∏—Ö —Ü–∏—Ç–∏—Ä–æ–≤–∞—Ç—å
            if self.knowledge_base:
                links = self.knowledge_base.get_file_links()
                if links:
                    logger.info(f"Adding {len(links)} document links to system instruction")
                    links_block = "\n### –°–°–´–õ–ö–ò –ù–ê –î–û–ö–£–ú–ï–ù–¢–´ –ë–ê–ó–´ –ó–ù–ê–ù–ò–ô (–î–õ–Ø –¶–ò–¢–ò–†–û–í–ê–ù–ò–Ø):\n"
                    for fname, url in links.items():
                        links_block += f"- {fname}: {url}\n"
                    links_block += "\n**–ü–†–ê–í–ò–õ–û:** –ï—Å–ª–∏ —Ç—ã –∏—Å–ø–æ–ª—å–∑—É–µ—à—å –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ñ–∞–π–ª–∞ –≤—ã—à–µ, –≤ –∫–æ–Ω—Ü–µ –æ—Ç–≤–µ—Ç–∞ –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –Ω–∞–ø–∏—à–∏: '–ü–æ–¥—Ä–æ–±–Ω–µ–µ —Å–º. –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ: [–ù–∞–∑–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞](—Å—Å—ã–ª–∫–∞)'."
                    effective_system_instruction += links_block

            config_params['system_instruction'] = effective_system_instruction
            config_params['tools'] = tools
            
            # –í–Ω–µ–¥—Ä—è–µ–º —Ñ–∞–π–ª—ã –∏–∑ KnowledgeBase –≤ –∏—Å—Ç–æ—Ä–∏—é, –µ—Å–ª–∏ –∫—ç—à –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è (–ø—Ä–æ—Å—Ç–æ–π RAG)
            # –í–Ω–µ–¥—Ä–µ–Ω–∏–µ –ø–æ–ª–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –æ—Ç–∫–ª—é—á–µ–Ω–æ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–≥—Ä—É–∑–∫–∏ API –∏ —Ç–∞–π–º–∞—É—Ç–æ–≤.
            # –í–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π RAG (–ø–µ—Ä–µ–¥–∞—á–∞ —Ç–æ–ª—å–∫–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ —Ç–µ–∫—Å—Ç–∞).
            pass

        else:
            config_params['cached_content'] = cache_name
        
        config = types.GenerateContentConfig(**config_params)
        generate_kwargs = {
            'model': effective_model,
            'contents': history,
            'config': config
        }

        # --- AUTO-RETRY LOGIC START ---
        MAX_RETRIES = 2
        full_reply_parts = []
        grounding_sources = {}
        
        for attempt in range(MAX_RETRIES + 1):
            full_reply_parts = [] # –°–±—Ä–æ—Å –±—É—Ñ–µ—Ä–æ–≤ –ø–µ—Ä–µ–¥ –Ω–æ–≤–æ–π –ø–æ–ø—ã—Ç–∫–æ–π
            grounding_sources = {}
            has_started_response = False # –§–ª–∞–≥: –Ω–∞—á–∞–ª–∏ –ª–∏ –º—ã —É–∂–µ –æ—Ç–¥–∞–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ
            
            try:
                logger.info(f"Starting Gemini stream for user {user_id} (Attempt {attempt+1}/{MAX_RETRIES+1})")
                
                # –¢–∞–π–º–∞—É—Ç –Ω–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é —Å—Ç—Ä–∏–º–∞ (60 —Å–µ–∫—É–Ω–¥)
                STREAM_INIT_TIMEOUT = 60.0
                try:
                    stream = await asyncio.wait_for(
                        client.aio.models.generate_content_stream(**generate_kwargs),
                        timeout=STREAM_INIT_TIMEOUT
                    )
                except asyncio.TimeoutError:
                    logger.error(f"Gemini stream init timeout ({STREAM_INIT_TIMEOUT}s) for user {user_id}")
                    raise TimeoutError(f"Gemini API –Ω–µ –æ—Ç–≤–µ—Ç–∏–ª –∑–∞ {STREAM_INIT_TIMEOUT} —Å–µ–∫—É–Ω–¥")
                
                async for response in stream:
                    
                    # –°–±–æ—Ä Grounding Metadata
                    if response.candidates and response.candidates[0].grounding_metadata:
                        gm = response.candidates[0].grounding_metadata
                        if gm.grounding_chunks:
                            for chunk in gm.grounding_chunks:
                                if chunk.web and chunk.web.uri and chunk.web.title:
                                    grounding_sources[chunk.web.uri] = chunk.web.title

                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ Function Call –≤ –ø–µ—Ä–≤–æ–º —á–∞–Ω–∫–µ
                    if response.candidates and response.candidates[0].content.parts:
                        part = response.candidates[0].content.parts[0]
                        
                        if part.function_call:
                            has_started_response = True # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏ —ç—Ç–æ –æ—Ç–≤–µ—Ç
                            fc = part.function_call
                            logger.info(f"–ò–ò –≤—ã–∑—ã–≤–∞–µ—Ç —Ñ—É–Ω–∫—Ü–∏—é (STREAM): {fc.name}")
                            
                            yield f"__TOOL_CALL__:{fc.name}"
                            
                            tool_result = "–î–∞–Ω–Ω—ã–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã"
                            if fc.name == 'get_promotions':
                                now = time.time()
                                if self._promotions_cache and (now - self._promotions_cache_time < self._promotions_cache_ttl):
                                    tool_result = self._promotions_cache
                                    logger.info("Using TTLCache for promotions")
                                else:
                                    if self.promotions_gateway:
                                        try:
                                            tool_result = await get_promotions_json(self.promotions_gateway)
                                            self._promotions_cache = tool_result
                                            self._promotions_cache_time = now
                                            logger.info(f"Promotions cache updated (len: {len(tool_result)})")
                                        except Exception as te:
                                            logger.error(f"Error calling promotion tool in stream: {te}")
                                    
                            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é
                            self.user_histories[user_id].append(response.candidates[0].content)
                            function_response_part = types.Part(
                                function_response=types.FunctionResponse(
                                    name=fc.name,
                                    response={'output': tool_result}
                                )
                            )
                            self.user_histories[user_id].append(types.Content(role="tool", parts=[function_response_part]))
                            
                            # RECURSION: –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–µ–º —Å—Ç—Ä–∏–º –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ —Ñ—É–Ω–∫—Ü–∏—é
                            # –ó–¥–µ—Å—å –≤–∞–∂–Ω–æ: —Ä–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –≤—ã–∑–æ–≤ ask_stream –±—É–¥–µ—Ç –∏–º–µ—Ç—å —Å–≤–æ–π —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π —Ü–∏–∫–ª retries!
                            async for sub_part in self.ask_stream(user_id, ""): 
                                if sub_part:
                                    yield sub_part
                            return # –ü–æ–ª–Ω—ã–π –≤—ã—Ö–æ–¥ –∏–∑ —Ç–µ–∫—É—â–µ–≥–æ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ (—É—Å–ø–µ—Ö)

                        # –ï—Å–ª–∏ —ç—Ç–æ –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç
                        if response.text:
                            text_chunk = response.text
                            # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –ø—Ä–∏—à–µ–ª, –∑–Ω–∞—á–∏—Ç —ç—Ç–æ –Ω–µ –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç
                            has_started_response = True
                            full_reply_parts.append(text_chunk)
                            yield text_chunk

                # –ö–æ–Ω–µ—Ü —Ü–∏–∫–ª–∞ —Å—Ç—Ä–∏–º–∏–Ω–≥–∞ –¥–ª—è –¥–∞–Ω–Ω–æ–π –ø–æ–ø—ã—Ç–∫–∏
                
                # –ö–ª—é—á–µ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –ë—ã–ª –ª–∏ –ø–æ–ª—É—á–µ–Ω –∫–∞–∫–æ–π-—Ç–æ —Ç–µ–∫—Å—Ç?
                if not full_reply_parts:
                    # –ï—Å–ª–∏ —Å—Ç—Ä–∏–º –∑–∞–≤–µ—Ä—à–∏–ª—Å—è –±–µ–∑ —Ç–µ–∫—Å—Ç–∞ –∏ –±–µ–∑ function call -> –≠—Ç–æ "Empty Response"
                    raise ValueError("Received empty stream response from Gemini model")
                
                # –ï—Å–ª–∏ –º—ã –∑–¥–µ—Å—å, –∑–Ω–∞—á–∏—Ç –æ—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω (full_reply_parts –Ω–µ –ø—É—Å—Ç), –≤—ã—Ö–æ–¥–∏–º –∏–∑ —Ü–∏–∫–ª–∞ retry
                break 

            except Exception as e:
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–∫–∏
                if has_started_response:
                    # –ï—Å–ª–∏ –º—ã —É–∂–µ –Ω–∞—á–∞–ª–∏ —Å—Ç—Ä–∏–º–∏—Ç—å —Ç–µ–∫—Å—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é, –º—ã –ù–ï –ú–û–ñ–ï–ú –¥–µ–ª–∞—Ç—å —Ä–µ—Ç—Ä–∞–π
                    # –∏–Ω–∞—á–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —É–≤–∏–¥–∏—Ç –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–ª–∏ –∫–∞—à—É.
                    # –ü—Ä–æ—Å—Ç–æ –ª–æ–≥–∏—Ä—É–µ–º –∏ –ø—Ä–µ—Ä—ã–≤–∞–µ–º.
                    logger.error(f"Stream error AFTER yield (user {user_id}): {e}")
                    yield f"\n[‚ö†Ô∏è –û–±—Ä—ã–≤ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è: {str(e)[:50]}]"
                    return # –ü—Ä–µ—Ä—ã–≤–∞–µ–º —Å—Ç—Ä–∏–º
                
                # –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –æ—à–∏–±–∫–∏ –∏—Å—Ç–µ–∫—à–µ–≥–æ –∫—ç—à–∞ –∏–ª–∏ —É—Å—Ç–∞—Ä–µ–≤—à–µ–≥–æ API
                error_str = str(e)
                
                # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–∫–∏ –∫–≤–æ—Ç—ã (429) - –Ω–µ –∂–¥–∞—Ç—å, –µ—Å–ª–∏ –ª–∏–º–∏—Ç –∏—Å—á–µ—Ä–ø–∞–Ω
                if '429' in error_str or 'RESOURCE_EXHAUSTED' in error_str:
                    logger.error(f"‚ùå Quota exceeded (429) for Gemini: {error_str}")
                    if attempt < MAX_RETRIES:
                        logger.warning("Quota hit, skipping backoff and trying next key/model immediately.")
                        # –ú—ã –ù–ï –∂–¥–µ–º, —Ç–∞–∫ –∫–∞–∫ 429 –æ–±—ã—á–Ω–æ –Ω–µ –ø—Ä–æ—Ö–æ–¥–∏—Ç –∑–∞ —Å–µ–∫—É–Ω–¥—ã
                        continue 
                    else:
                        raise e # –ü—Ä–æ–±—Ä–∞—Å—ã–≤–∞–µ–º –≤—ã—à–µ –¥–ª—è –ø–µ—Ä–µ—Ö–æ–¥–∞ –∫ OpenRouter

                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—à–∏–±–∫–∏ –∫—ç—à–∞: –∏—Å—Ç–µ–∫—à–∏–π, –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π, –∏–ª–∏ —É—Å—Ç–∞—Ä–µ–≤—à–∏–π API
                if ('CachedContent' in error_str and ('403' in error_str or 'PERMISSION_DENIED' in error_str)) or \
                   'google_search' in error_str or \
                   'not supported' in error_str.lower():
                    logger.warning(f"‚ùå Cache error or outdated API: {e}")
                    # –ò–Ω–≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞—Ç—å –∫—ç—à –≤ Knowledge Base
                    await self.knowledge_base.invalidate_cache()
                    # –ü–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å config –ë–ï–ó –∫—ç—à–∞ –¥–ª—è –ø–æ–≤—Ç–æ—Ä–∞
                    config_params['system_instruction'] = self.system_instruction
                    config_params['tools'] = tools
                    if 'cached_content' in config_params:
                        del config_params['cached_content']
                    config = types.GenerateContentConfig(**config_params)
                    generate_kwargs['config'] = config
                
                if attempt < MAX_RETRIES:
                    # Exponential Backoff: 1s, 2s, 4s...
                    wait_time = (2 ** attempt) + 0.1
                    logger.info(f"üîÑ Retrying Gemini in {wait_time}s")
                    await asyncio.sleep(wait_time) 
                    continue # –ò–¥–µ–º –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π –∫—Ä—É–≥
                else:
                    # –í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –¥–ª—è Gemini –∏—Å—á–µ—Ä–ø–∞–Ω—ã
                    logger.error(f"All {MAX_RETRIES+1} attempts failed for user {user_id}")
                    raise e

        # --- FINALIZATION (Success case) ---
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –±–ª–æ–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (Grounding)
        if grounding_sources:
            sources_text = "\n\nüìö **–ò—Å—Ç–æ—á–Ω–∏–∫–∏:**\n"
            for i, (uri, title) in enumerate(grounding_sources.items(), 1):
                sources_text += f"{i}. [{title}]({uri})\n"
            
            yield sources_text
            full_reply_parts.append(sources_text)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –≤ –∏—Å—Ç–æ—Ä–∏—é
        if full_reply_parts:
            full_reply = "".join(full_reply_parts)
            self._add_to_history(user_id, "model", full_reply)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ SQLite (–ö–†–ò–¢–ò–ß–ù–û –¥–ª—è –ø–∞–º—è—Ç–∏)
            asyncio.create_task(self.sqlite_memory.add_message(user_id, "model", full_reply))
            
            # –ú–µ—Ç—Ä–∏–∫–∏ LLM: –ª–æ–≥–∏—Ä—É–µ–º –≤—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞
            llm_duration_ms = (time.perf_counter() - llm_start_time) * 1000
            log_llm_metrics(
                user_id=user_id,
                model=effective_model,
                duration_ms=llm_duration_ms,
                success=True
            )
            logger.info(f"Stream finished for user {user_id}, history updated. Sources: {len(grounding_sources)}, Duration: {llm_duration_ms:.0f}ms")
            
            # –ê—Ä—Ö–∏–≤–∞—Ü–∏—è –∏—Å—Ç–æ—Ä–∏–∏ –¥–ª—è "–ø–∞–º—è—Ç–∏"
            if self.memory_archiver:
                 asyncio.create_task(self.memory_archiver.archive_user_history(
                     user_id, 
                     self.user_histories.get(user_id, [])
                 ))

    async def ask(self, user_id: int, content: str, external_history: Optional[str] = None) -> Optional[str]:
        """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∑–∞–ø—Ä–æ—Å –≤ Gemini –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç (—á–µ—Ä–µ–∑ —Å—Ç—Ä–∏–º–∏–Ω–≥)."""
        full_reply_parts = []
        async for part in self.ask_stream(user_id, content, external_history):
            full_reply_parts.append(part)
        return "".join(full_reply_parts) if full_reply_parts else None

    async def _ask_stream_openrouter(self, user_id: int, content: str, external_history: Optional[str] = None) -> AsyncGenerator[str, None]:
        """–í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –º–µ—Ç–æ–¥ –¥–ª—è —Å—Ç—Ä–∏–º–∏–Ω–≥–∞ —á–µ—Ä–µ–∑ OpenRouter."""
        MAX_OR_RETRIES = len(self.or_models)
        full_reply = ""
        current_or_model = "unknown"
        
        for attempt in range(MAX_OR_RETRIES):
            try:
                messages = []
                if self.system_instruction:
                    messages.append({"role": "system", "content": self.system_instruction})
                
                # --- Rate Limiter (Basic) ---
                # –î–∞–µ–º –ø–∞—É–∑—É –ø–µ—Ä–µ–¥ –∑–∞–ø—Ä–æ—Å–∞–º–∏ –∫ OpenRouter —á—Ç–æ–±—ã –Ω–µ –ª–æ–≤–∏—Ç—å 429
                # –û—Å–æ–±–µ–Ω–Ω–æ –≤–∞–∂–Ω–æ –¥–ª—è –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
                last_req_time = getattr(self, '_last_request_time', 0)
                time_since_last = time.time() - last_req_time
                if time_since_last < 2.0: # –ú–∏–Ω–∏–º—É–º 2 —Å–µ–∫—É–Ω–¥—ã –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                    sleep_time = 2.0 - time_since_last
                    logger.info(f"Rate limit protection: sleeping {sleep_time:.2f}s")
                    await asyncio.sleep(sleep_time)
                self._last_request_time = time.time()
                # -----------------------------

                if self.knowledge_base:
                    links = self.knowledge_base.get_file_links()
                    if links:
                        links_block = "\n### –°–°–´–õ–ö–ò –ù–ê –î–û–ö–£–ú–ï–ù–¢–´ –ë–ê–ó–´ –ó–ù–ê–ù–ò–ô:\n"
                        for fname, url in links.items():
                            links_block += f"- {fname}: {url}\n"
                        messages[0]["content"] += links_block

                if external_history and external_history.strip():
                    clean_history = external_history[-3000:]
                    messages.append({"role": "user", "content": f"–ö—Ä–∞—Ç–∫–∞—è –∏—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞:\n{clean_history}"})
                    messages.append({"role": "assistant", "content": "–ü–æ–Ω—è–ª, —É—á–∏—Ç—ã–≤–∞—é –∏—Å—Ç–æ—Ä–∏—é."})

                messages.append({"role": "user", "content": content})

                current_or_model = self.or_models[self.current_or_model_index]
                logger.info(f"OpenRouter stream for user {user_id} with model {current_or_model} (Attempt {attempt+1}/{MAX_OR_RETRIES})")

                response = await self.or_client.chat.completions.create(
                    model=current_or_model,
                    messages=messages,
                    stream=True,
                    temperature=0.7
                )

                async for chunk in response:
                    if chunk.choices[0].delta.content:
                        text = chunk.choices[0].delta.content
                        full_reply += text
                        yield text
                
                if full_reply:
                    break
                else:
                    raise ValueError("Received empty response from OpenRouter model")

            except Exception as e:
                logger.error(f"OpenRouter Error with model {current_or_model}: {e}")
                if attempt < MAX_OR_RETRIES - 1:
                    logger.warning(f"Rotating OpenRouter index and retrying (attempt {attempt+1}/{MAX_OR_RETRIES})")
                    self.current_or_model_index = (self.current_or_model_index + 1) % len(self.or_models)
                    await asyncio.sleep(1) 
                else:
                    logger.error(f"All {MAX_OR_RETRIES} OpenRouter models failed")
                    raise e 

        if self.memory_archiver and full_reply:
            fake_history = [
                types.Content(role="user", parts=[types.Part(text=content)]),
                types.Content(role="model", parts=[types.Part(text=full_reply)])
            ]
            asyncio.create_task(self.memory_archiver.archive_user_history(user_id, fake_history))

    def clear_history(self, user_id: int) -> None:
        """–û—á–∏—â–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è."""
        if user_id in self.user_histories:
            del self.user_histories[user_id]
            logger.info(f"Cleared chat history for user {user_id}")
    async def wait_for_ready(self):
        """–û–∂–∏–¥–∞–µ—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π."""
        while self._initializing:
            await asyncio.sleep(0.5)
